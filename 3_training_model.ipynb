{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f605c3d3",
   "metadata": {},
   "source": [
    "# Training Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4313a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7b6d1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/feature_engineering/loans_train.csv\")\n",
    "test = pd.read_csv(\"data/feature_engineering/loans_test.csv\")\n",
    "valid = pd.read_csv(\"data/feature_engineering/loans_valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc5cc6",
   "metadata": {},
   "source": [
    "# Training Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8bbc8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Datetime Columns Before Training (concise)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATE_COLS = ['FirstPaymentDate', 'MaturityDate']\n",
    "\n",
    "def extract_true_datetime_parts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dfc = df.copy()\n",
    "    for col in DATE_COLS:\n",
    "        if col in dfc.columns:\n",
    "            dfc[col] = pd.to_datetime(dfc[col], format='%Y-%m-%d', errors='coerce')\n",
    "            dfc[f'{col}_year'] = dfc[col].dt.year\n",
    "            dfc[f'{col}_month'] = dfc[col].dt.month\n",
    "            dfc.drop(columns=[col], inplace=True)\n",
    "    return dfc\n",
    "\n",
    "train = extract_true_datetime_parts(train)\n",
    "valid = extract_true_datetime_parts(valid)\n",
    "test  = extract_true_datetime_parts(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3a14197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Datetime extraction completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Extract YYYY, MM, DD from Datetime Columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_datetime_features(df):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for col in ['FirstPaymentDate', 'MaturityDate']:\n",
    "        if col in df_copy.columns:\n",
    "            # Safely convert to datetime if not already\n",
    "            df_copy[col] = pd.to_datetime(df_copy[col], errors='coerce')\n",
    "\n",
    "            # Extract components using datetime accessor\n",
    "            df_copy[col + '_year'] = df_copy[col].dt.year\n",
    "            df_copy[col + '_month'] = df_copy[col].dt.month\n",
    "            df_copy[col + '_day'] = df_copy[col].dt.day\n",
    "\n",
    "            # Drop original datetime column\n",
    "            df_copy.drop(columns=[col], inplace=True)\n",
    "            print(f\"Extracted year, month, day from {col}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Apply extraction to all datasets\n",
    "train = extract_datetime_features(train)\n",
    "valid = extract_datetime_features(valid)\n",
    "test  = extract_datetime_features(test)\n",
    "\n",
    "print(\"\\nâœ… Datetime extraction completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "229e4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop all original panel columns (0_...13_...)\n",
    "panel_cols = [c for c in train.columns if any(c.startswith(f\"{i}_\") for i in range(14))]\n",
    "train = train.drop(columns=panel_cols, errors='ignore')\n",
    "valid = valid.drop(columns=panel_cols, errors='ignore')\n",
    "test  = test.drop(columns=panel_cols,  errors='ignore')\n",
    "\n",
    "# 2. Keep aggregated statistical features\n",
    "# (e.g. EstimatedLTV_mean, EstimatedLTV_slope, LoanAge_range, etc.)\n",
    "X_train = train.drop(['index', 'target'], axis=1)\n",
    "y_train = train['target']\n",
    "X_valid = valid.drop(['index', 'target'], axis=1)\n",
    "y_valid = valid['target']\n",
    "X_test  = test.drop(['Id'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "72778c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Cleaned Datasets for Model Training\n",
    "\n",
    "from signal import valid_signals\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Use your cleaned datasets that already have datetime features extracted\n",
    "X_train = train.drop(columns=['index', 'target'], errors='ignore')\n",
    "y_train = train['target']\n",
    "X_valid = valid.drop(columns=['index', 'target'], errors='ignore')\n",
    "y_valid = valid['target']\n",
    "X_test = test.drop(columns=['Id'], errors='ignore')\n",
    "\n",
    "# Ensure all datasets have the same columns\n",
    "common_cols = set(X_train.columns) & set(X_valid.columns) & set(X_test.columns)\n",
    "X_train = X_train[list(common_cols)]\n",
    "X_valid = X_valid[list(common_cols)]\n",
    "X_test = X_test[list(common_cols)]\n",
    "\n",
    "# Drop all string/categorical columns (union across splits)\n",
    "obj_cols = set(X_train.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols |= set(X_valid.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols |= set(X_test.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols = list(obj_cols)\n",
    "\n",
    "X_train = X_train.drop(columns=obj_cols, errors='ignore')\n",
    "X_valid = X_valid.drop(columns=obj_cols, errors='ignore')\n",
    "X_test  = X_test.drop(columns=obj_cols, errors='ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f07df095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Fill remaining NaN values\n",
      "Step 3: Verify fix results\n",
      "NaN count in X_train: 0\n",
      "NaN count in X_valid: 0\n",
      "NaN count in X_test: 0\n"
     ]
    }
   ],
   "source": [
    "# Fill remaining NaN values using robust methods\n",
    "\n",
    "print(\"Step 2: Fill remaining NaN values\")\n",
    "\n",
    "# For numeric columns, use median imputation\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if X_train[col].isna().any():\n",
    "        median_val = X_train[col].median()\n",
    "        if pd.isna(median_val):  # If median is also NaN, use 0\n",
    "            median_val = 0\n",
    "        X_train[col] = X_train[col].fillna(median_val)\n",
    "        X_valid[col] = X_valid[col].fillna(median_val)\n",
    "        X_test[col] = X_test[col].fillna(median_val)\n",
    "\n",
    "# For categorical columns, use mode imputation\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_cols:\n",
    "    if X_train[col].isna().any():\n",
    "        mode_val = X_train[col].mode()\n",
    "        if len(mode_val) > 0:\n",
    "            X_train[col] = X_train[col].fillna(mode_val[0])\n",
    "            X_valid[col] = X_valid[col].fillna(mode_val[0])\n",
    "            X_test[col] = X_test[col].fillna(mode_val[0])\n",
    "        else:\n",
    "            # If no mode exists, use 'unknown'\n",
    "            X_train[col] = X_train[col].fillna('unknown')\n",
    "            X_valid[col] = X_valid[col].fillna('unknown')\n",
    "            X_test[col] = X_test[col].fillna('unknown')\n",
    "\n",
    "# 3. Verify the fix results\n",
    "print(\"Step 3: Verify fix results\")\n",
    "print(f\"NaN count in X_train: {X_train.isna().sum().sum()}\")\n",
    "print(f\"NaN count in X_valid: {X_valid.isna().sum().sum()}\")\n",
    "print(f\"NaN count in X_test: {X_test.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4dfc77",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d5b93fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ æ­£åœ¨æ‰§è¡Œç‰¹å¾é€‰æ‹©...\n",
      "   (ä½¿ç”¨ RandomForestClassifier ä½œä¸º'ç›‘ç£è€…'ï¼Œåœ¨ valid æ•°æ®é›†ä¸Šå¯»æ‰¾é‡è¦ç‰¹å¾...)\n",
      "âœ… å·²é€‰å‡º Top 75 ä¸ªç‰¹å¾ã€‚\n",
      "æœ€é‡è¦çš„5ä¸ªç‰¹å¾: ['InterestBearingUPB_num_decrease', 'CurrentActualUPB_late_slope', 'CurrentActualUPB_num_decrease', 'CreditScore', 'InterestBearingUPB_late_slope']\n",
      "   â†’ å·²åˆ›å»º X_train_selectedï¼Œå½¢çŠ¶: (30504, 50)\n",
      "   â†’ å·²åˆ›å»º X_valid_selectedï¼Œå½¢çŠ¶: (5370, 50)\n",
      "   â†’ å·²åˆ›å»º X_train_normal_selected (ç”¨äºè®­ç»ƒ)ï¼Œå½¢çŠ¶: (30504, 50)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸš€ æ­£åœ¨æ‰§è¡Œç‰¹å¾é€‰æ‹©...\")\n",
    "print(\"   (ä½¿ç”¨ RandomForestClassifier ä½œä¸º'ç›‘ç£è€…'ï¼Œåœ¨ valid æ•°æ®é›†ä¸Šå¯»æ‰¾é‡è¦ç‰¹å¾...)\")\n",
    "\n",
    "# 1. å®šä¹‰ä¸€ä¸ªç›‘ç£æ¨¡å‹ä½œä¸ºâ€œç‰¹å¾é€‰æ‹©å™¨â€\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# 2. åœ¨ *éªŒè¯é›†* (Valid) ä¸Šè®­ç»ƒå®ƒ\n",
    "#    æˆ‘ä»¬ä½¿ç”¨ X_valid_scaled å’Œ y_valid_\n",
    "#    (è¿™äº›å˜é‡æ˜¯åœ¨æ‚¨ 3_training_model.ipynb çš„ id: 84e8d18f å•å…ƒæ ¼ä¸­å®šä¹‰çš„)\n",
    "rf_selector.fit(X_valid_scaled, y_valid_) \n",
    "\n",
    "# 3. è·å–ç‰¹å¾é‡è¦æ€§\n",
    "importances = rf_selector.feature_importances_\n",
    "\n",
    "# 4. è·å–ç‰¹å¾åç§°\n",
    "#    æˆ‘ä»¬å¿…é¡»ä½¿ç”¨ *å®Œå…¨ç›¸åŒ* çš„ DataFrame (X_train_) æ¥è·å–åˆ—åï¼Œ\n",
    "#    è¿™ä¸ª X_train_ æ˜¯æ‚¨åœ¨ id: 84e8d18f ä¸­ç”¨æ¥ .fit(scaler) çš„é‚£ä¸ª\n",
    "feature_names = X_train_.columns \n",
    "\n",
    "# 5. åˆ›å»ºé‡è¦æ€§ DataFrame\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# 6. é€‰å‡º Top 75 (æˆ– 100) ä¸ªæœ€é‡è¦çš„ç‰¹å¾\n",
    "TOP_N_FEATURES = 75\n",
    "top_features = feature_importance_df.head(TOP_N_FEATURES)['feature'].tolist()\n",
    "\n",
    "print(f\"âœ… å·²é€‰å‡º Top {TOP_N_FEATURES} ä¸ªç‰¹å¾ã€‚\")\n",
    "print(\"æœ€é‡è¦çš„5ä¸ªç‰¹å¾:\", top_features[:5])\n",
    "\n",
    "# 7. åˆ›å»º *åªåŒ…å«* è¿™äº›ç‰¹å¾çš„æ–°æ•°æ®é›†\n",
    "#    (æˆ‘ä»¬å¿…é¡»ä»å·²ç¼©æ”¾çš„ Numpy æ•°ç»„ä¸­è¿›è¡Œç­›é€‰)\n",
    "\n",
    "# a. å…ˆå°† Numpy æ•°ç»„è½¬å›å¸¦åˆ—åçš„ DataFrame\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "X_valid_scaled_df = pd.DataFrame(X_valid_scaled, columns=feature_names)\n",
    "\n",
    "# b. ç°åœ¨ï¼Œç”¨ top_features åˆ—è¡¨æ¥ç­›é€‰\n",
    "X_train_selected = X_train_scaled_df[top_features]\n",
    "X_valid_selected = X_valid_scaled_df[top_features]\n",
    "\n",
    "print(f\"   â†’ å·²åˆ›å»º X_train_selectedï¼Œå½¢çŠ¶: {X_train_selected.shape}\")\n",
    "print(f\"   â†’ å·²åˆ›å»º X_valid_selectedï¼Œå½¢çŠ¶: {X_valid_selected.shape}\")\n",
    "\n",
    "# 8. (é‡è¦) ä¸ºæ— ç›‘ç£è®­ç»ƒåˆ›å»ºâ€œä»…æ­£å¸¸â€çš„å­é›†\n",
    "#    æˆ‘ä»¬ä½¿ç”¨ y_train_ (æ¥è‡ª id: 84e8d18f) æ¥ç­›é€‰è¡Œ\n",
    "X_train_normal_selected = X_train_selected[y_train_ == 0]\n",
    "print(f\"   â†’ å·²åˆ›å»º X_train_normal_selected (ç”¨äºè®­ç»ƒ)ï¼Œå½¢çŠ¶: {X_train_normal_selected.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "340f3f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_selected.copy()\n",
    "X_valid = X_valid_selected.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ac6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰©å±•Baselineæ¨¡å‹ - æ·»åŠ æ›´å¤šæ— ç›‘ç£æ–¹æ³•\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# ---------- æ•°æ®å‡†å¤‡ ----------\n",
    "X_train_ = X_train.copy()\n",
    "y_train_ = y_train.copy()\n",
    "X_valid_ = X_valid.copy()\n",
    "y_valid_ = y_valid.copy()\n",
    "\n",
    "# æ ‡å‡†åŒ–\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_)\n",
    "X_train_scaled = scaler.transform(X_train_)\n",
    "X_valid_scaled = scaler.transform(X_valid_)\n",
    "\n",
    "# ---------- å®šä¹‰æ‰©å±•æ¨¡å‹ ----------\n",
    "models = {\n",
    "    # åŸæœ‰æ–¹æ³•\n",
    "    \"IsolationForest\": IsolationForest(\n",
    "        n_estimators=200,\n",
    "        contamination=0.05,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"LocalOutlierFactor\": LocalOutlierFactor(\n",
    "        n_neighbors=10,\n",
    "        novelty=True,\n",
    "        contamination=0.05\n",
    "    ),\n",
    "    \"PCA_Reconstruction\": PCA(n_components=0.95, random_state=42),\n",
    "    \"OneClassSVM\": OneClassSVM(\n",
    "        kernel=\"rbf\",\n",
    "        gamma=\"scale\",\n",
    "        nu=0.05\n",
    "    ),\n",
    "    \n",
    "    # æ–°å¢æ–¹æ³•\n",
    "    \"EllipticEnvelope\": EllipticEnvelope(\n",
    "        contamination=0.05,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"DBSCAN\": DBSCAN(\n",
    "        eps=0.5,\n",
    "        min_samples=5\n",
    "    )\n",
    "}\n",
    "\n",
    "# ---------- è¯„ä¼° ----------\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == \"PCA_Reconstruction\":\n",
    "        model.fit(X_train_scaled)\n",
    "        Xv_rec = model.inverse_transform(model.transform(X_valid_scaled))\n",
    "        scores = np.mean((X_valid_scaled - Xv_rec) ** 2, axis=1)\n",
    "    elif name == \"DBSCAN\":\n",
    "        # DBSCANéœ€è¦ç‰¹æ®Šå¤„ç†\n",
    "        labels = model.fit_predict(X_train_scaled)\n",
    "        # ä½¿ç”¨è®­ç»ƒå¥½çš„èšç±»ä¸­å¿ƒè®¡ç®—éªŒè¯é›†çš„è·ç¦»\n",
    "        unique_labels = set(labels)\n",
    "        if -1 in unique_labels:\n",
    "            unique_labels.remove(-1)  # ç§»é™¤å™ªå£°æ ‡ç­¾\n",
    "        \n",
    "        if len(unique_labels) > 0:\n",
    "            # è®¡ç®—æ¯ä¸ªèšç±»ä¸­å¿ƒ\n",
    "            cluster_centers = []\n",
    "            for label in unique_labels:\n",
    "                cluster_data = X_train_scaled[labels == label]\n",
    "                center = np.mean(cluster_data, axis=0)\n",
    "                cluster_centers.append(center)\n",
    "            \n",
    "            # è®¡ç®—éªŒè¯é›†åˆ°æœ€è¿‘èšç±»ä¸­å¿ƒçš„è·ç¦»\n",
    "            scores = []\n",
    "            for point in X_valid_scaled:\n",
    "                distances = [np.linalg.norm(point - center) for center in cluster_centers]\n",
    "                scores.append(min(distances))\n",
    "            scores = np.array(scores)\n",
    "        else:\n",
    "            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆèšç±»ï¼Œä½¿ç”¨åˆ°è®­ç»ƒé›†ä¸­å¿ƒçš„è·ç¦»\n",
    "            center = np.mean(X_train_scaled, axis=0)\n",
    "            scores = np.array([np.linalg.norm(point - center) for point in X_valid_scaled])\n",
    "    else:\n",
    "        model.fit(X_train_scaled)\n",
    "        scores = -model.decision_function(X_valid_scaled)\n",
    "\n",
    "    ap = average_precision_score(y_valid_, scores)\n",
    "    roc = roc_auc_score(y_valid_, scores)\n",
    "    results.append((name, round(ap, 4), round(roc, 4)))\n",
    "\n",
    "# ---------- è¾“å‡ºç»“æœ ----------\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"AP\", \"ROC-AUC\"]).sort_values(\"AP\", ascending=False)\n",
    "print(\"âœ… æ‰©å±•Baselineç»“æœ:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoEncoderæ–¹æ³• - æ·±åº¦é‡æ„ç½‘ç»œ\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import Input, Dense\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    \n",
    "    print(\"ğŸš€ æ­£åœ¨è®­ç»ƒAutoEncoder...\")\n",
    "    \n",
    "    # æ„å»ºAutoEncoder\n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "    encoding_dim = min(32, input_dim // 2)  # ç¼–ç ç»´åº¦\n",
    "    \n",
    "    # ç¼–ç å™¨\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(encoding_dim, activation=\"relu\")(input_layer)\n",
    "    encoder = Dense(encoding_dim // 2, activation=\"relu\")(encoder)\n",
    "    \n",
    "    # è§£ç å™¨\n",
    "    decoder = Dense(encoding_dim, activation=\"relu\")(encoder)\n",
    "    decoder = Dense(input_dim, activation=\"sigmoid\")(decoder)\n",
    "    \n",
    "    # å®Œæ•´æ¨¡å‹\n",
    "    autoencoder = Model(input_layer, decoder)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    \n",
    "    # è®­ç»ƒ\n",
    "    autoencoder.fit(\n",
    "        X_train_scaled, X_train_scaled,\n",
    "        epochs=50,\n",
    "        batch_size=256,\n",
    "        validation_split=0.1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # è®¡ç®—é‡æ„è¯¯å·®\n",
    "    X_valid_pred = autoencoder.predict(X_valid_scaled, verbose=0)\n",
    "    reconstruction_error = np.mean((X_valid_scaled - X_valid_pred) ** 2, axis=1)\n",
    "    \n",
    "    # è¯„ä¼°\n",
    "    ap_ae = average_precision_score(y_valid_, reconstruction_error)\n",
    "    roc_ae = roc_auc_score(y_valid_, reconstruction_error)\n",
    "    \n",
    "    print(f\"âœ… AutoEncoderç»“æœ: AP={ap_ae:.4f}, ROC-AUC={roc_ae:.4f}\")\n",
    "    \n",
    "    # æ·»åŠ åˆ°ç»“æœä¸­\n",
    "    results.append((\"AutoEncoder\", round(ap_ae, 4), round(roc_ae, 4)))\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš ï¸ TensorFlowæœªå®‰è£…ï¼Œè·³è¿‡AutoEncoder\")\n",
    "    print(\"å®‰è£…å‘½ä»¤: pip install tensorflow\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ AutoEncoderè®­ç»ƒå¤±è´¥: {e}\")\n",
    "\n",
    "# æ›´æ–°æœ€ç»ˆç»“æœ\n",
    "final_results_df = pd.DataFrame(results, columns=[\"Model\", \"AP\", \"ROC-AUC\"]).sort_values(\"AP\", ascending=False)\n",
    "print(f\"\\nğŸ¯ æœ€ç»ˆBaselineç»“æœ (å…±{len(results)}ä¸ªæ¨¡å‹):\")\n",
    "print(final_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8f2ac",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c5f4135f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid positive ratio: 0.1261\n"
     ]
    }
   ],
   "source": [
    "# Print target=1 ratio in valid (concise)\n",
    "print(f\"valid positive ratio: {y_valid.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8d18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Unsupervised Baseline Results (Full Train):\n",
      "                Model      AP  ROC-AUC\n",
      "1  LocalOutlierFactor  0.2717   0.6640\n",
      "3         OneClassSVM  0.2113   0.6454\n",
      "2  PCA_Reconstruction  0.1586   0.5505\n",
      "0     IsolationForest  0.1549   0.6079\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Unsupervised anomaly detection baseline (Full-train version)\n",
    "# Train: ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ ·æœ¬ (åŒ…å« target=0/1)\n",
    "# Valid: æ··åˆæ ·æœ¬ï¼Œç”¨äºè¯„ä¼°\n",
    "# Metrics: AP (main), ROC-AUC (secondary)\n",
    "# ===========================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "\n",
    "# ---------- æ•°æ®å‡†å¤‡ ----------\n",
    "X_train_ = X_train.copy()\n",
    "y_train_ = y_train.copy()\n",
    "X_valid_ = X_valid.copy()\n",
    "y_valid_ = y_valid.copy()\n",
    "\n",
    "# æ ‡å‡†åŒ–ï¼ˆä¿æŒ pipeline ç»“æ„ç»Ÿä¸€ï¼‰\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_)\n",
    "X_train_scaled = scaler.transform(X_train_)\n",
    "X_valid_scaled = scaler.transform(X_valid_)\n",
    "\n",
    "# ---------- å®šä¹‰æ¨¡å‹ ----------\n",
    "models = {\n",
    "    \"IsolationForest\": IsolationForest(\n",
    "        n_estimators=200,\n",
    "        contamination=0.05,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"LocalOutlierFactor\": LocalOutlierFactor(\n",
    "        n_neighbors=10,\n",
    "        novelty=True,\n",
    "        contamination=0.05\n",
    "    ),\n",
    "    \"PCA_Reconstruction\": PCA(n_components=0.95, random_state=42),\n",
    "    \"OneClassSVM\": OneClassSVM(\n",
    "        kernel=\"rbf\",\n",
    "        gamma=\"scale\",\n",
    "        nu=0.05\n",
    "    )\n",
    "}\n",
    "\n",
    "# ---------- è¯„ä¼° ----------\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == \"PCA_Reconstruction\":\n",
    "        model.fit(X_train_scaled)\n",
    "        Xv_rec = model.inverse_transform(model.transform(X_valid_scaled))\n",
    "        scores = np.mean((X_valid_scaled - Xv_rec) ** 2, axis=1)\n",
    "    else:\n",
    "        model.fit(X_train_scaled)\n",
    "        scores = -model.decision_function(X_valid_scaled)\n",
    "\n",
    "    ap = average_precision_score(y_valid_, scores)\n",
    "    roc = roc_auc_score(y_valid_, scores)\n",
    "    results.append((name, round(ap, 4), round(roc, 4)))\n",
    "\n",
    "# ---------- è¾“å‡ºç»“æœ ----------\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"AP\", \"ROC-AUC\"]).sort_values(\"AP\", ascending=False)\n",
    "print(\"âœ… Unsupervised Baseline Results (Full Train):\")\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "is5126",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
