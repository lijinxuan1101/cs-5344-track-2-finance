{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f605c3d3",
   "metadata": {},
   "source": [
    "# Training Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4313a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7b6d1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/feature_engineering/loans_train.csv\")\n",
    "test = pd.read_csv(\"data/feature_engineering/loans_test.csv\")\n",
    "valid = pd.read_csv(\"data/feature_engineering/loans_valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc5cc6",
   "metadata": {},
   "source": [
    "### training model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8bbc8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Datetime Columns Before Training (concise)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATE_COLS = ['FirstPaymentDate', 'MaturityDate']\n",
    "\n",
    "def extract_true_datetime_parts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dfc = df.copy()\n",
    "    for col in DATE_COLS:\n",
    "        if col in dfc.columns:\n",
    "            dfc[col] = pd.to_datetime(dfc[col], format='%Y-%m-%d', errors='coerce')\n",
    "            dfc[f'{col}_year'] = dfc[col].dt.year\n",
    "            dfc[f'{col}_month'] = dfc[col].dt.month\n",
    "            dfc.drop(columns=[col], inplace=True)\n",
    "    return dfc\n",
    "\n",
    "train = extract_true_datetime_parts(train)\n",
    "valid = extract_true_datetime_parts(valid)\n",
    "test  = extract_true_datetime_parts(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a14197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Datetime extraction completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Extract YYYY, MM, DD from Datetime Columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_datetime_features(df):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for col in ['FirstPaymentDate', 'MaturityDate']:\n",
    "        if col in df_copy.columns:\n",
    "            # Safely convert to datetime if not already\n",
    "            df_copy[col] = pd.to_datetime(df_copy[col], errors='coerce')\n",
    "\n",
    "            # Extract components using datetime accessor\n",
    "            df_copy[col + '_year'] = df_copy[col].dt.year\n",
    "            df_copy[col + '_month'] = df_copy[col].dt.month\n",
    "            df_copy[col + '_day'] = df_copy[col].dt.day\n",
    "\n",
    "            # Drop original datetime column\n",
    "            df_copy.drop(columns=[col], inplace=True)\n",
    "            print(f\"Extracted year, month, day from {col}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Apply extraction to all datasets\n",
    "train = extract_datetime_features(train)\n",
    "valid = extract_datetime_features(valid)\n",
    "test  = extract_datetime_features(test)\n",
    "\n",
    "print(\"\\n✅ Datetime extraction completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "229e4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop all original panel columns (0_...13_...)\n",
    "panel_cols = [c for c in train.columns if any(c.startswith(f\"{i}_\") for i in range(14))]\n",
    "train = train.drop(columns=panel_cols, errors='ignore')\n",
    "valid = valid.drop(columns=panel_cols, errors='ignore')\n",
    "test  = test.drop(columns=panel_cols,  errors='ignore')\n",
    "\n",
    "# 2. Keep aggregated statistical features\n",
    "# (e.g. EstimatedLTV_mean, EstimatedLTV_slope, LoanAge_range, etc.)\n",
    "X_train = train.drop(['index', 'target'], axis=1)\n",
    "y_train = train['target']\n",
    "X_valid = valid.drop(['index', 'target'], axis=1)\n",
    "y_valid = valid['target']\n",
    "X_test  = test.drop(['Id'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "72778c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Cleaned Datasets for Model Training\n",
    "\n",
    "from signal import valid_signals\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Use your cleaned datasets that already have datetime features extracted\n",
    "X_train = train.drop(columns=['index', 'target'], errors='ignore')\n",
    "y_train = train['target']\n",
    "X_valid = valid.drop(columns=['index', 'target'], errors='ignore')\n",
    "y_valid = valid['target']\n",
    "X_test = test.drop(columns=['Id'], errors='ignore')\n",
    "\n",
    "# Ensure all datasets have the same columns\n",
    "common_cols = set(X_train.columns) & set(X_valid.columns) & set(X_test.columns)\n",
    "X_train = X_train[list(common_cols)]\n",
    "X_valid = X_valid[list(common_cols)]\n",
    "X_test = X_test[list(common_cols)]\n",
    "\n",
    "# Drop all string/categorical columns (union across splits)\n",
    "obj_cols = set(X_train.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols |= set(X_valid.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols |= set(X_test.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols = list(obj_cols)\n",
    "\n",
    "X_train = X_train.drop(columns=obj_cols, errors='ignore')\n",
    "X_valid = X_valid.drop(columns=obj_cols, errors='ignore')\n",
    "X_test  = X_test.drop(columns=obj_cols, errors='ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f07df095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Fill remaining NaN values\n",
      "Step 3: Verify fix results\n",
      "NaN count in X_train: 0\n",
      "NaN count in X_valid: 0\n",
      "NaN count in X_test: 0\n"
     ]
    }
   ],
   "source": [
    "# Fill remaining NaN values using robust methods\n",
    "\n",
    "print(\"Step 2: Fill remaining NaN values\")\n",
    "\n",
    "# For numeric columns, use median imputation\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if X_train[col].isna().any():\n",
    "        median_val = X_train[col].median()\n",
    "        if pd.isna(median_val):  # If median is also NaN, use 0\n",
    "            median_val = 0\n",
    "        X_train[col] = X_train[col].fillna(median_val)\n",
    "        X_valid[col] = X_valid[col].fillna(median_val)\n",
    "        X_test[col] = X_test[col].fillna(median_val)\n",
    "\n",
    "# For categorical columns, use mode imputation\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_cols:\n",
    "    if X_train[col].isna().any():\n",
    "        mode_val = X_train[col].mode()\n",
    "        if len(mode_val) > 0:\n",
    "            X_train[col] = X_train[col].fillna(mode_val[0])\n",
    "            X_valid[col] = X_valid[col].fillna(mode_val[0])\n",
    "            X_test[col] = X_test[col].fillna(mode_val[0])\n",
    "        else:\n",
    "            # If no mode exists, use 'unknown'\n",
    "            X_train[col] = X_train[col].fillna('unknown')\n",
    "            X_valid[col] = X_valid[col].fillna('unknown')\n",
    "            X_test[col] = X_test[col].fillna('unknown')\n",
    "\n",
    "# 3. Verify the fix results\n",
    "print(\"Step 3: Verify fix results\")\n",
    "print(f\"NaN count in X_train: {X_train.isna().sum().sum()}\")\n",
    "print(f\"NaN count in X_valid: {X_valid.isna().sum().sum()}\")\n",
    "print(f\"NaN count in X_test: {X_test.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8f2ac",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c5f4135f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid positive ratio: 0.1261\n"
     ]
    }
   ],
   "source": [
    "# Print target=1 ratio in valid (concise)\n",
    "print(f\"valid positive ratio: {y_valid.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "84e8d18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Unsupervised Baseline Results (Full Train):\n",
      "                Model      AP  ROC-AUC\n",
      "1  LocalOutlierFactor  0.2567   0.6363\n",
      "2  PCA_Reconstruction  0.2326   0.6011\n",
      "3         OneClassSVM  0.2048   0.6026\n",
      "0     IsolationForest  0.1337   0.5491\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Unsupervised anomaly detection baseline (Full-train version)\n",
    "# Train: 使用全部训练样本 (包含 target=0/1)\n",
    "# Valid: 混合样本，用于评估\n",
    "# Metrics: AP (main), ROC-AUC (secondary)\n",
    "# ===========================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "# ---------- 数据准备 ----------\n",
    "X_train_ = X_train.copy()\n",
    "y_train_ = y_train.copy()\n",
    "X_valid_ = X_valid.copy()\n",
    "y_valid_ = y_valid.copy()\n",
    "\n",
    "# 标准化（保持 pipeline 结构统一）\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_)\n",
    "X_train_scaled = scaler.transform(X_train_)\n",
    "X_valid_scaled = scaler.transform(X_valid_)\n",
    "\n",
    "# ---------- 定义模型 ----------\n",
    "models = {\n",
    "    \"IsolationForest\": IsolationForest(\n",
    "        n_estimators=200,\n",
    "        contamination=0.05,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"LocalOutlierFactor\": LocalOutlierFactor(\n",
    "        n_neighbors=10,\n",
    "        novelty=True,\n",
    "        contamination=0.05\n",
    "    ),\n",
    "    \"PCA_Reconstruction\": PCA(n_components=0.95, random_state=42),\n",
    "    \"OneClassSVM\": OneClassSVM(\n",
    "        kernel=\"rbf\",\n",
    "        gamma=\"scale\",\n",
    "        nu=0.05\n",
    "    )\n",
    "}\n",
    "\n",
    "# ---------- 评估 ----------\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == \"PCA_Reconstruction\":\n",
    "        model.fit(X_train_scaled)\n",
    "        Xv_rec = model.inverse_transform(model.transform(X_valid_scaled))\n",
    "        scores = np.mean((X_valid_scaled - Xv_rec) ** 2, axis=1)\n",
    "    else:\n",
    "        model.fit(X_train_scaled)\n",
    "        scores = -model.decision_function(X_valid_scaled)\n",
    "\n",
    "    ap = average_precision_score(y_valid_, scores)\n",
    "    roc = roc_auc_score(y_valid_, scores)\n",
    "    results.append((name, round(ap, 4), round(roc, 4)))\n",
    "\n",
    "# ---------- 输出结果 ----------\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"AP\", \"ROC-AUC\"]).sort_values(\"AP\", ascending=False)\n",
    "print(\"✅ Unsupervised Baseline Results (Full Train):\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "600653b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_neighbors=10 | AP=0.2567\n",
      "n_neighbors=20 | AP=0.2507\n",
      "n_neighbors=30 | AP=0.2491\n",
      "n_neighbors=50 | AP=0.2455\n"
     ]
    }
   ],
   "source": [
    "for n in [10, 20, 30, 50]:\n",
    "    lof = LocalOutlierFactor(n_neighbors=n, novelty=True, contamination=0.05)\n",
    "    lof.fit(scaler.transform(X_train_[y_train_==0]))\n",
    "    scores = -lof.decision_function(X_valid_scaled)\n",
    "    ap = average_precision_score(y_valid_, scores)\n",
    "    print(f\"n_neighbors={n} | AP={ap:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "28d3612a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble — AP=0.2507, ROC-AUC=0.6309\n"
     ]
    }
   ],
   "source": [
    "lof = LocalOutlierFactor(n_neighbors=20, novelty=True, contamination=0.05)\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "\n",
    "lof.fit(X_train_scaled[y_train_==0])\n",
    "pca.fit(X_train_scaled[y_train_==0])\n",
    "\n",
    "# 得分融合（标准化后求均值）\n",
    "lof_score = -lof.decision_function(X_valid_scaled)\n",
    "pca_score = np.mean((X_valid_scaled - pca.inverse_transform(pca.transform(X_valid_scaled)))**2, axis=1)\n",
    "scores = (lof_score / np.std(lof_score)) + (pca_score / np.std(pca_score))\n",
    "\n",
    "ap = average_precision_score(y_valid_, scores)\n",
    "roc = roc_auc_score(y_valid_, scores)\n",
    "print(f\"Ensemble — AP={ap:.4f}, ROC-AUC={roc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "is5126",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
