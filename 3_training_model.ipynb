{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f605c3d3",
   "metadata": {},
   "source": [
    "# Training Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4313a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b6d1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/feature_engineering/loans_train.csv\")\n",
    "test = pd.read_csv(\"data/feature_engineering/loans_test.csv\")\n",
    "valid = pd.read_csv(\"data/feature_engineering/loans_valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc5cc6",
   "metadata": {},
   "source": [
    "# Training Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72778c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Fill remaining NaN values\n",
      "NaN count in X_train: 52690\n",
      "NaN count in X_valid: 9584\n",
      "NaN count in X_test: 22635\n",
      "Found 298 numeric columns for imputation.\n",
      "Median imputation complete.\n",
      "Step 3: Verify fix results\n",
      "NaN count in X_train: 0\n",
      "NaN count in X_valid: 0\n",
      "NaN count in X_test: 0\n"
     ]
    }
   ],
   "source": [
    "# Use Cleaned Datasets for Model Training\n",
    "\n",
    "from signal import valid_signals\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Use your cleaned datasets that already have datetime features extracted\n",
    "train = train[train['target'] == 0]\n",
    "X_train = train.drop(columns=['index', 'target'], errors='ignore')\n",
    "y_train = train['target']\n",
    "X_valid = valid.drop(columns=['index', 'target'], errors='ignore')\n",
    "y_valid = valid['target']\n",
    "X_test = test.drop(columns=['Id'], errors='ignore')\n",
    "\n",
    "# Ensure all datasets have the same columns\n",
    "common_cols = set(X_train.columns) & set(X_valid.columns) & set(X_test.columns)\n",
    "X_train = X_train[list(common_cols)]\n",
    "X_valid = X_valid[list(common_cols)]\n",
    "X_test = X_test[list(common_cols)]\n",
    "\n",
    "# Drop all string/categorical columns (union across splits)\n",
    "obj_cols = set(X_train.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols |= set(X_valid.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols |= set(X_test.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols = list(obj_cols)\n",
    "\n",
    "X_train = X_train.drop(columns=obj_cols, errors='ignore')\n",
    "X_valid = X_valid.drop(columns=obj_cols, errors='ignore')\n",
    "X_test  = X_test.drop(columns=obj_cols, errors='ignore')\n",
    "\n",
    "\n",
    "# Fill remaining NaN values using robust methods\n",
    "print(\"Step 2: Fill remaining NaN values\")\n",
    "print(f\"NaN count in X_train: {X_train.isna().sum().sum()}\")\n",
    "print(f\"NaN count in X_valid: {X_valid.isna().sum().sum()}\")\n",
    "print(f\"NaN count in X_test: {X_test.isna().sum().sum()}\")\n",
    "\n",
    "# For numeric columns, use median imputation\n",
    "# 1. æ‰¾å‡ºæ‰€æœ‰æ•°å€¼ç±»å‹çš„åˆ—\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "print(f\"Found {len(numeric_cols)} numeric columns for imputation.\")\n",
    "# 2. éå†æ¯ä¸€åˆ—è¿›è¡Œä¸­ä½æ•°å¡«å……\n",
    "for col in numeric_cols:\n",
    "    # æ£€æŸ¥è®­ç»ƒé›†ä¸­æ˜¯å¦æœ‰ NaN\n",
    "    if X_train[col].isna().any():\n",
    "        # 2a. ä»…ä»è®­ç»ƒé›†ä¸­è®¡ç®—ä¸­ä½æ•°\n",
    "        median_val = X_train[col].median()\n",
    "        # 2b. å¤„ç†è¾¹ç¼˜æƒ…å†µï¼šå¦‚æœè®­ç»ƒé›†çš„è¯¥åˆ—å…¨æ˜¯ NaNï¼Œåˆ™ä¸­ä½æ•°ä¹Ÿæ˜¯ NaN\n",
    "        if pd.isna(median_val):\n",
    "            median_val = 0  # é»˜è®¤å¡«å……ä¸º 0\n",
    "        # 2c. ä½¿ç”¨è®¡ç®—å‡ºçš„ä¸­ä½æ•°å¡«å……æ‰€æœ‰æ•°æ®é›†\n",
    "        X_train[col] = X_train[col].fillna(median_val)\n",
    "        X_valid[col] = X_valid[col].fillna(median_val)\n",
    "        X_test[col] = X_test[col].fillna(median_val)\n",
    "print(\"Median imputation complete.\")\n",
    "\n",
    "# For categorical columns, use mode imputation\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_cols:\n",
    "    if X_train[col].isna().any():\n",
    "        mode_val = X_train[col].mode()\n",
    "        if len(mode_val) > 0:\n",
    "            X_train[col] = X_train[col].fillna(mode_val[0])\n",
    "            X_valid[col] = X_valid[col].fillna(mode_val[0])\n",
    "            X_test[col] = X_test[col].fillna(mode_val[0])\n",
    "        else:\n",
    "            # If no mode exists, use 'unknown'\n",
    "            X_train[col] = X_train[col].fillna('unknown')\n",
    "            X_valid[col] = X_valid[col].fillna('unknown')\n",
    "            X_test[col] = X_test[col].fillna('unknown')\n",
    "\n",
    "# 3. Verify the fix results\n",
    "print(\"Step 3: Verify fix results\")\n",
    "print(f\"NaN count in X_train: {X_train.isna().sum().sum()}\")\n",
    "print(f\"NaN count in X_valid: {X_valid.isna().sum().sum()}\")\n",
    "print(f\"NaN count in X_test: {X_test.isna().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4dfc77",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b93fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Executing Feature Selection...\n",
      "   (Using RandomForestClassifier trained on the validation set to find important features...)\n",
      "âœ… Selected Top 75 features.\n",
      "Top 5 most important features: ['InterestBearingUPB_num_decrease', 'CurrentActualUPB_late_slope', 'InterestBearingUPB_late_std', 'CurrentActualUPB_num_decrease', 'CreditScore']\n",
      "   â†’ Created X_train_selected with shape: (30504, 75)\n",
      "   â†’ Created X_valid_selected with shape: (5370, 75)\n",
      "   â†’ Created X_test_selected with shape: (13426, 75)\n",
      "   â†’ Created X_train_normal_selected (for training) with shape: (30504, 75)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "# Ensure consistent columns before scaling (important!)\n",
    "common_cols = list(set(X_train.columns) & set(X_valid.columns))\n",
    "X_train = X_train[common_cols]\n",
    "X_valid = X_valid[common_cols]\n",
    "X_test = X_test[common_cols]\n",
    "\n",
    "# Drop non-numeric columns AFTER identifying common columns\n",
    "object_cols = X_train.select_dtypes(include=['object']).columns\n",
    "X_train = X_train.drop(columns=object_cols)\n",
    "X_valid = X_valid.drop(columns=object_cols)\n",
    "X_test = X_test.drop(columns=object_cols)\n",
    "\n",
    "# Store feature names BEFORE scaling\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"ğŸš€ Executing Feature Selection...\")\n",
    "print(\"   (Using RandomForestClassifier trained on the validation set to find important features...)\")\n",
    "\n",
    "# Define the selector model\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train the selector on the VALIDATION data (X_valid_scaled, y_valid)\n",
    "# This uses the labels (0s and 1s) in the validation set to determine feature importance\n",
    "rf_selector.fit(X_valid_scaled, y_valid)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_selector.feature_importances_\n",
    "\n",
    "# Check if lengths match (should match if scaler and fit used correctly)\n",
    "if len(feature_names) == len(importances):\n",
    "    # Create importance DataFrame\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    # Select Top N features\n",
    "    TOP_N_FEATURES = 75 # You can adjust this number\n",
    "    top_features = feature_importance_df.head(TOP_N_FEATURES)['feature'].tolist()\n",
    "\n",
    "    print(f\"âœ… Selected Top {TOP_N_FEATURES} features.\")\n",
    "    print(\"Top 5 most important features:\", top_features[:5])\n",
    "\n",
    "    # Create new DataFrames/arrays containing only the selected features\n",
    "    # Need to convert scaled arrays back to DataFrame temporarily to select by name\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "    X_valid_scaled_df = pd.DataFrame(X_valid_scaled, columns=feature_names)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "\n",
    "    # final selected features df\n",
    "    X_train_selected = X_train_scaled_df[top_features]\n",
    "    X_valid_selected = X_valid_scaled_df[top_features]\n",
    "    X_test_selected = X_test_scaled_df[top_features]\n",
    "\n",
    "\n",
    "    # Create the 'normal only' training set for unsupervised models\n",
    "    # Convert y_train to numpy array if it's a pandas Series for boolean indexing\n",
    "    y_train_np = y_train.to_numpy() if isinstance(y_train, pd.Series) else y_train\n",
    "    X_train_normal_selected = X_train_selected[y_train_np == 0]\n",
    "\n",
    "    print(f\"   â†’ Created X_train_selected with shape: {X_train_selected.shape}\")\n",
    "    print(f\"   â†’ Created X_valid_selected with shape: {X_valid_selected.shape}\")\n",
    "    print(f\"   â†’ Created X_test_selected with shape: {X_test_selected.shape}\")\n",
    "    print(f\"   â†’ Created X_train_normal_selected (for training) with shape: {X_train_normal_selected.shape}\")\n",
    "\n",
    "else:\n",
    "    print(\"--- ERROR ---\")\n",
    "    print(\"Length mismatch between feature names and importances.\")\n",
    "    print(f\"Length of feature_names (from X_train before scaling): {len(feature_names)}\")\n",
    "    print(f\"Length of importances (from RandomForest): {len(importances)}\")\n",
    "    print(\"This indicates an issue in data preparation before scaling or in the selector training.\")\n",
    "    print(\"Please check the steps before this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8f2ac",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5f4135f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid positive ratio: 0.1261\n"
     ]
    }
   ],
   "source": [
    "# Print target=1 ratio in valid (concise)\n",
    "print(f\"valid positive ratio: {y_valid.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a4397db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Running Baseline Models (Using Top 75 Selected & Scaled Features)...\n",
      "   â†’ Using 30504 normal training samples for fitting (with 75 features).\n",
      "   â†’ Evaluating on 5370 validation samples (with 75 features).\n",
      "    Fitting IsolationForest...\n",
      "     IsolationForest -> AP: 0.1414, ROC-AUC: 0.5779\n",
      "    Fitting LocalOutlierFactor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sixestates/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LocalOutlierFactor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     LocalOutlierFactor -> AP: 0.2803, ROC-AUC: 0.6743\n",
      "    Fitting PCA_Reconstruction...\n",
      "     PCA_Reconstruction -> AP: 0.1868, ROC-AUC: 0.5884\n",
      "    Fitting OneClassSVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sixestates/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     OneClassSVM -> AP: 0.2061, ROC-AUC: 0.6497\n",
      "\n",
      "âœ… Baseline Results (Top 75 Scaled Features, Trained on Normal Samples Only):\n",
      "                Model      AP  ROC-AUC\n",
      "1  LocalOutlierFactor  0.2803   0.6743\n",
      "3         OneClassSVM  0.2061   0.6497\n",
      "2  PCA_Reconstruction  0.1868   0.5884\n",
      "0     IsolationForest  0.1414   0.5779\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# Baseline Model Evaluation (Using TOP N Selected Features - CORRECTED)\n",
    "# =============================================================\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# No StandardScaler needed here as selected features are already scaled\n",
    "\n",
    "# Check if feature selection variables exist\n",
    "if 'TOP_N_FEATURES' not in locals() or \\\n",
    "   'X_train_selected' not in locals() or \\\n",
    "   'X_valid_selected' not in locals() or \\\n",
    "   'y_train' not in locals() or \\\n",
    "   'y_valid' not in locals():\n",
    "    print(\"--- ERROR ---\")\n",
    "    print(\"Required variables (TOP_N_FEATURES, X_train_selected, X_valid_selected, y_train, y_valid) not found.\")\n",
    "    print(\"Please ensure you have successfully run the 'Feature Selection' cell (id: d5b93fe1) immediately before this cell.\")\n",
    "else:\n",
    "    print(f\"ğŸš€ Running Baseline Models (Using Top {TOP_N_FEATURES} Selected & Scaled Features)...\")\n",
    "\n",
    "    # ---------- Data Preparation ----------\n",
    "    # Use variables from feature selection cell (already scaled)\n",
    "    X_train_ = X_train_selected.copy()\n",
    "    X_valid_ = X_valid_selected.copy()\n",
    "    y_train_ = y_train.copy() # Should contain only 0s\n",
    "    y_valid_ = y_valid.copy() # Contains 0s and 1s\n",
    "\n",
    "    # (===== â­ï¸ Crucial Step: Create the 'normal only' subset for fitting =====)\n",
    "    X_train_fit_normal = X_train_[y_train_ == 0]\n",
    "\n",
    "    print(f\"   â†’ Using {X_train_fit_normal.shape[0]} normal training samples for fitting (with {X_train_fit_normal.shape[1]} features).\")\n",
    "    print(f\"   â†’ Evaluating on {X_valid_.shape[0]} validation samples (with {X_valid_.shape[1]} features).\")\n",
    "\n",
    "    # ---------- Define Models ----------\n",
    "    # Hyperparameters might need re-tuning based on the selected features\n",
    "    models = {\n",
    "        \"IsolationForest\": IsolationForest(\n",
    "            n_estimators=200,\n",
    "            contamination=0.05, # Hyperparameter to tune\n",
    "            random_state=42\n",
    "        ),\n",
    "        \"LocalOutlierFactor\": LocalOutlierFactor(\n",
    "            n_neighbors=10,       # Hyperparameter to tune\n",
    "            novelty=True,\n",
    "            contamination=0.05  # Hyperparameter to tune\n",
    "        ),\n",
    "        \"PCA_Reconstruction\": PCA(\n",
    "            n_components=0.95,\n",
    "            random_state=42     # n_components might need tuning\n",
    "        ),\n",
    "        \"OneClassSVM\": OneClassSVM(\n",
    "            kernel=\"rbf\",\n",
    "            gamma=\"scale\",\n",
    "            nu=0.05             # Hyperparameter to tune\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # ---------- Evaluation Loop ----------\n",
    "    results_selected = [] # Use a distinct list name\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"    Fitting {name}...\")\n",
    "        scores = None\n",
    "        try:\n",
    "            # (===== â­ï¸ Corrected Fit Call =====)\n",
    "            # Fit ALL models ONLY on normal samples\n",
    "            model.fit(X_train_fit_normal)\n",
    "\n",
    "            # (===== â­ï¸ Corrected Evaluation Call =====)\n",
    "            # Evaluate ALL models on the full validation set (X_valid_)\n",
    "            if name == \"PCA_Reconstruction\":\n",
    "                # Ensure X_valid_ is NumPy for calculation if it's a DataFrame\n",
    "                X_valid_np = X_valid_.values if isinstance(X_valid_, pd.DataFrame) else X_valid_\n",
    "                Xv_rec = model.inverse_transform(model.transform(X_valid_np))\n",
    "                scores = np.mean((X_valid_np - Xv_rec) ** 2, axis=1) # Higher error = more anomalous\n",
    "            else: # IF, LOF, OCSVM\n",
    "                # Ensure X_valid_ is compatible with decision_function (DataFrame is usually fine)\n",
    "                scores = -model.decision_function(X_valid_) # Higher score = more anomalous\n",
    "\n",
    "            # --- Calculate Metrics ---\n",
    "            if scores is None or np.any(np.isnan(scores)) or np.any(np.isinf(scores)):\n",
    "                 valid_scores = scores[~(np.isnan(scores) | np.isinf(scores))] if scores is not None else np.array([0.0])\n",
    "                 median_score = np.median(valid_scores) if len(valid_scores) > 0 else 0.0\n",
    "                 scores = np.nan_to_num(scores, nan=median_score, posinf=median_score, neginf=median_score)\n",
    "\n",
    "            if np.sum(y_valid_) == 0 or np.all(y_valid_ == y_valid_[0]):\n",
    "                 ap, roc = np.nan, np.nan\n",
    "            else:\n",
    "                 ap = average_precision_score(y_valid_, scores)\n",
    "                 roc = roc_auc_score(y_valid_, scores)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"     ERROR fitting or predicting {name}: {e}\")\n",
    "            ap, roc = np.nan, np.nan\n",
    "\n",
    "        results_selected.append((name, round(ap, 4), round(roc, 4)))\n",
    "        print(f\"     {name} -> AP: {ap:.4f}, ROC-AUC: {roc:.4f}\")\n",
    "\n",
    "    # ---------- Output Results ----------\n",
    "    results_df_selected = pd.DataFrame(results_selected, columns=[\"Model\", \"AP\", \"ROC-AUC\"]).sort_values(\"AP\", ascending=False)\n",
    "    print(f\"\\nâœ… Baseline Results (Top {X_train_fit_normal.shape[1]} Scaled Features, Trained on Normal Samples Only):\")\n",
    "    print(results_df_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fe5dc6",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba88582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¡¹ç›®è¦æ±‚ä¸‹çš„æ­£ç¡®ä»£ç ç»“æ„\n",
    "print(\"ğŸ“‹ é¡¹ç›®çº¦æŸè¯´æ˜ï¼š\")\n",
    "print(\"1. åªèƒ½åœ¨ target=0 çš„æ•°æ®ä¸Šè®­ç»ƒ\")\n",
    "print(\"2. å¯ä»¥ä½¿ç”¨ valid é›†è¿›è¡Œç‰¹å¾ç­›é€‰\")\n",
    "print(\"3. æ¶ˆé™¤ sklearn ç‰¹å¾åç§°è­¦å‘Š\")\n",
    "\n",
    "# é‡æ–°ç»„ç»‡æ•°æ®æµç¨‹\n",
    "print(\"\\nğŸ”„ é‡æ–°ç»„ç»‡æ•°æ®æµç¨‹...\")\n",
    "\n",
    "# 1. ä¿ç•™å®Œæ•´æ•°æ®é›†\n",
    "train_full = train.copy()\n",
    "valid_full = valid.copy() \n",
    "test_full = test.copy()\n",
    "\n",
    "# 2. åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾\n",
    "X_train_full = train_full.drop(columns=['index', 'target'], errors='ignore')\n",
    "y_train_full = train_full['target']\n",
    "X_valid_full = valid_full.drop(columns=['index', 'target'], errors='ignore')\n",
    "y_valid_full = valid_full['target']\n",
    "X_test_full = test_full.drop(columns=['Id'], errors='ignore')\n",
    "\n",
    "# 3. ç¡®ä¿åˆ—å¯¹é½\n",
    "common_cols = set(X_train_full.columns) & set(X_valid_full.columns) & set(X_test_full.columns)\n",
    "X_train_full = X_train_full[list(common_cols)]\n",
    "X_valid_full = X_valid_full[list(common_cols)]\n",
    "X_test_full = X_test_full[list(common_cols)]\n",
    "\n",
    "# 4. åˆ é™¤éæ•°å€¼åˆ—\n",
    "obj_cols = set(X_train_full.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols |= set(X_valid_full.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols |= set(X_test_full.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols = list(obj_cols)\n",
    "\n",
    "X_train_full = X_train_full.drop(columns=obj_cols, errors='ignore')\n",
    "X_valid_full = X_valid_full.drop(columns=obj_cols, errors='ignore')\n",
    "X_test_full = X_test_full.drop(columns=obj_cols, errors='ignore')\n",
    "\n",
    "# 5. å¡«å……ç¼ºå¤±å€¼\n",
    "print(\"å¡«å……ç¼ºå¤±å€¼...\")\n",
    "numeric_cols = X_train_full.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if X_train_full[col].isna().any():\n",
    "        median_val = X_train_full[col].median()\n",
    "        if pd.isna(median_val):\n",
    "            median_val = 0\n",
    "        X_train_full[col] = X_train_full[col].fillna(median_val)\n",
    "        X_valid_full[col] = X_valid_full[col].fillna(median_val)\n",
    "        X_test_full[col] = X_test_full[col].fillna(median_val)\n",
    "\n",
    "# 6. é¡¹ç›®çº¦æŸï¼šåªä½¿ç”¨target=0çš„è®­ç»ƒæ•°æ®\n",
    "train_mask = y_train_full == 0\n",
    "X_train = X_train_full[train_mask].copy()\n",
    "y_train = y_train_full[train_mask].copy()\n",
    "X_valid = X_valid_full.copy()\n",
    "y_valid = y_valid_full.copy()\n",
    "X_test = X_test_full.copy()\n",
    "\n",
    "print(f\"\\nâœ… æ•°æ®å‡†å¤‡å®Œæˆï¼š\")\n",
    "print(f\"   - è®­ç»ƒé›†ï¼ˆtarget=0ï¼‰: {X_train.shape}\")\n",
    "print(f\"   - éªŒè¯é›†: {X_valid.shape}\")\n",
    "print(f\"   - æµ‹è¯•é›†: {X_test.shape}\")\n",
    "print(f\"   - éªŒè¯é›†æ­£æ ·æœ¬æ¯”ä¾‹: {y_valid.mean():.4f}\")\n",
    "\n",
    "# 7. æ ‡å‡†åŒ–å¹¶ä¿æŒDataFrameæ ¼å¼\n",
    "print(\"\\nğŸ”§ æ ‡å‡†åŒ–æ•°æ®...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train), \n",
    "    columns=X_train.columns, \n",
    "    index=X_train.index\n",
    ")\n",
    "X_valid_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_valid), \n",
    "    columns=X_valid.columns, \n",
    "    index=X_valid.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test), \n",
    "    columns=X_test.columns, \n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(\"âœ… æ ‡å‡†åŒ–å®Œæˆï¼Œä¿æŒDataFrameæ ¼å¼\")\n",
    "\n",
    "# 8. ä½¿ç”¨éªŒè¯é›†è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼ˆé¡¹ç›®è¦æ±‚ï¼‰\n",
    "print(\"\\nğŸ¯ ä½¿ç”¨éªŒè¯é›†è¿›è¡Œç‰¹å¾é€‰æ‹©...\")\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_selector.fit(X_valid_scaled, y_valid)\n",
    "\n",
    "importances = rf_selector.feature_importances_\n",
    "feature_names = X_train_scaled.columns.tolist()\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "TOP_N_FEATURES = 75\n",
    "top_features = feature_importance_df.head(TOP_N_FEATURES)['feature'].tolist()\n",
    "\n",
    "print(f\"âœ… é€‰æ‹©äº†Top {TOP_N_FEATURES}ä¸ªç‰¹å¾\")\n",
    "print(\"Top 5ä¸ªæœ€é‡è¦çš„ç‰¹å¾:\", top_features[:5])\n",
    "\n",
    "# 9. åˆ›å»ºæœ€ç»ˆæ•°æ®é›†\n",
    "X_train_selected = X_train_scaled[top_features]\n",
    "X_valid_selected = X_valid_scaled[top_features]\n",
    "X_test_selected = X_test_scaled[top_features]\n",
    "\n",
    "# è®­ç»ƒæ—¶åªä½¿ç”¨target=0çš„æ•°æ®ï¼ˆå·²ç»æ˜¯äº†ï¼‰\n",
    "X_train_normal_selected = X_train_selected.copy()\n",
    "\n",
    "print(f\"\\nğŸ‰ æœ€ç»ˆæ•°æ®é›†ï¼š\")\n",
    "print(f\"   - è®­ç»ƒé›†ï¼ˆtarget=0ï¼‰: {X_train_normal_selected.shape}\")\n",
    "print(f\"   - éªŒè¯é›†: {X_valid_selected.shape}\")\n",
    "print(f\"   - æµ‹è¯•é›†: {X_test_selected.shape}\")\n",
    "print(f\"   - æ‰€æœ‰æ•°æ®éƒ½æ˜¯DataFrameæ ¼å¼ï¼Œé¿å…ç‰¹å¾åç§°è­¦å‘Š\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29343b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿®å¤åçš„åŸºçº¿æ¨¡å‹æµ‹è¯•ï¼ˆæ¶ˆé™¤ç‰¹å¾åç§°è­¦å‘Šï¼‰\n",
    "print(\"ğŸ§ª æµ‹è¯•ä¿®å¤åçš„åŸºçº¿æ¨¡å‹...\")\n",
    "\n",
    "# æ£€æŸ¥å˜é‡æ˜¯å¦å­˜åœ¨\n",
    "if 'X_train_normal_selected' in locals() and 'X_valid_selected' in locals():\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.svm import OneClassSVM\n",
    "    from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "    \n",
    "    # ç¡®ä¿æ•°æ®æ ¼å¼ä¸€è‡´ï¼ˆéƒ½æ˜¯DataFrameï¼‰\n",
    "    X_train_fit = X_train_normal_selected.copy()\n",
    "    X_valid_eval = X_valid_selected.copy()\n",
    "    \n",
    "    print(f\"âœ… æ•°æ®æ ¼å¼æ£€æŸ¥:\")\n",
    "    print(f\"   - X_train_fit type: {type(X_train_fit)}\")\n",
    "    print(f\"   - X_valid_eval type: {type(X_valid_eval)}\")\n",
    "    print(f\"   - X_train_fit has columns: {hasattr(X_train_fit, 'columns')}\")\n",
    "    print(f\"   - X_valid_eval has columns: {hasattr(X_valid_eval, 'columns')}\")\n",
    "    \n",
    "    # å®šä¹‰æ¨¡å‹\n",
    "    models = {\n",
    "        \"IsolationForest\": IsolationForest(n_estimators=200, contamination=0.05, random_state=42),\n",
    "        \"LocalOutlierFactor\": LocalOutlierFactor(n_neighbors=10, contamination=0.05, novelty=True),\n",
    "        \"PCA_Reconstruction\": PCA(n_components=0.95, random_state=42),\n",
    "        \"OneClassSVM\": OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.05)\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"   æµ‹è¯• {name}...\")\n",
    "        try:\n",
    "            # è®­ç»ƒæ¨¡å‹\n",
    "            model.fit(X_train_fit)\n",
    "            \n",
    "            # é¢„æµ‹\n",
    "            if name == \"PCA_Reconstruction\":\n",
    "                Xv_rec = model.inverse_transform(model.transform(X_valid_eval))\n",
    "                scores = np.mean((X_valid_eval - Xv_rec) ** 2, axis=1)\n",
    "            else:\n",
    "                scores = -model.decision_function(X_valid_eval)\n",
    "            \n",
    "            # è®¡ç®—æŒ‡æ ‡\n",
    "            ap = average_precision_score(y_valid, scores)\n",
    "            roc = roc_auc_score(y_valid, scores)\n",
    "            \n",
    "            results.append((name, round(ap, 4), round(roc, 4)))\n",
    "            print(f\"     {name} -> AP: {ap:.4f}, ROC-AUC: {roc:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     {name} å¤±è´¥: {e}\")\n",
    "            results.append((name, np.nan, np.nan))\n",
    "    \n",
    "    # è¾“å‡ºç»“æœ\n",
    "    results_df = pd.DataFrame(results, columns=[\"Model\", \"AP\", \"ROC-AUC\"]).sort_values(\"AP\", ascending=False)\n",
    "    print(f\"\\nâœ… ä¿®å¤åçš„åŸºçº¿ç»“æœï¼ˆæ— ç‰¹å¾åç§°è­¦å‘Šï¼‰:\")\n",
    "    print(results_df)\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ æ‰€éœ€å˜é‡ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œä¸Šä¸€ä¸ªcell\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "is5126",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
