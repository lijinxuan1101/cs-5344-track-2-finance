{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f605c3d3",
   "metadata": {},
   "source": [
    "# Training Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4313a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b6d1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/feature_engineering/loans_train.csv\")\n",
    "test = pd.read_csv(\"data/feature_engineering/loans_test.csv\")\n",
    "valid = pd.read_csv(\"data/feature_engineering/loans_valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc5cc6",
   "metadata": {},
   "source": [
    "# Training Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72778c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Fill remaining NaN values\n",
      "NaN count in X_train: 52690\n",
      "NaN count in X_valid: 9584\n",
      "NaN count in X_test: 22635\n",
      "Found 298 numeric columns for imputation.\n",
      "Median imputation complete.\n",
      "Step 3: Verify fix results\n",
      "NaN count in X_train: 0\n",
      "NaN count in X_valid: 0\n",
      "NaN count in X_test: 0\n"
     ]
    }
   ],
   "source": [
    "# Use Cleaned Datasets for Model Training\n",
    "\n",
    "from signal import valid_signals\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Use your cleaned datasets that already have datetime features extracted\n",
    "train = train[train['target'] == 0]\n",
    "X_train = train.drop(columns=['index', 'target'], errors='ignore')\n",
    "y_train = train['target']\n",
    "X_valid = valid.drop(columns=['index', 'target'], errors='ignore')\n",
    "y_valid = valid['target']\n",
    "X_test = test.drop(columns=['Id'], errors='ignore')\n",
    "\n",
    "# Ensure all datasets have the same columns\n",
    "common_cols = set(X_train.columns) & set(X_valid.columns) & set(X_test.columns)\n",
    "X_train = X_train[list(common_cols)]\n",
    "X_valid = X_valid[list(common_cols)]\n",
    "X_test = X_test[list(common_cols)]\n",
    "\n",
    "# Drop all string/categorical columns (union across splits)\n",
    "obj_cols = set(X_train.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols |= set(X_valid.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols |= set(X_test.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols = list(obj_cols)\n",
    "\n",
    "X_train = X_train.drop(columns=obj_cols, errors='ignore')\n",
    "X_valid = X_valid.drop(columns=obj_cols, errors='ignore')\n",
    "X_test  = X_test.drop(columns=obj_cols, errors='ignore')\n",
    "\n",
    "\n",
    "# Fill remaining NaN values using robust methods\n",
    "print(\"Step 2: Fill remaining NaN values\")\n",
    "print(f\"NaN count in X_train: {X_train.isna().sum().sum()}\")\n",
    "print(f\"NaN count in X_valid: {X_valid.isna().sum().sum()}\")\n",
    "print(f\"NaN count in X_test: {X_test.isna().sum().sum()}\")\n",
    "\n",
    "# For numeric columns, use median imputation\n",
    "# 1. æ‰¾å‡ºæ‰€æœ‰æ•°å€¼ç±»åž‹çš„åˆ—\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "print(f\"Found {len(numeric_cols)} numeric columns for imputation.\")\n",
    "# 2. éåŽ†æ¯ä¸€åˆ—è¿›è¡Œä¸­ä½æ•°å¡«å……\n",
    "for col in numeric_cols:\n",
    "    # æ£€æŸ¥è®­ç»ƒé›†ä¸­æ˜¯å¦æœ‰ NaN\n",
    "    if X_train[col].isna().any():\n",
    "        # 2a. ä»…ä»Žè®­ç»ƒé›†ä¸­è®¡ç®—ä¸­ä½æ•°\n",
    "        median_val = X_train[col].median()\n",
    "        # 2b. å¤„ç†è¾¹ç¼˜æƒ…å†µï¼šå¦‚æžœè®­ç»ƒé›†çš„è¯¥åˆ—å…¨æ˜¯ NaNï¼Œåˆ™ä¸­ä½æ•°ä¹Ÿæ˜¯ NaN\n",
    "        if pd.isna(median_val):\n",
    "            median_val = 0  # é»˜è®¤å¡«å……ä¸º 0\n",
    "        # 2c. ä½¿ç”¨è®¡ç®—å‡ºçš„ä¸­ä½æ•°å¡«å……æ‰€æœ‰æ•°æ®é›†\n",
    "        X_train[col] = X_train[col].fillna(median_val)\n",
    "        X_valid[col] = X_valid[col].fillna(median_val)\n",
    "        X_test[col] = X_test[col].fillna(median_val)\n",
    "print(\"Median imputation complete.\")\n",
    "\n",
    "# For categorical columns, use mode imputation\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_cols:\n",
    "    if X_train[col].isna().any():\n",
    "        mode_val = X_train[col].mode()\n",
    "        if len(mode_val) > 0:\n",
    "            X_train[col] = X_train[col].fillna(mode_val[0])\n",
    "            X_valid[col] = X_valid[col].fillna(mode_val[0])\n",
    "            X_test[col] = X_test[col].fillna(mode_val[0])\n",
    "        else:\n",
    "            # If no mode exists, use 'unknown'\n",
    "            X_train[col] = X_train[col].fillna('unknown')\n",
    "            X_valid[col] = X_valid[col].fillna('unknown')\n",
    "            X_test[col] = X_test[col].fillna('unknown')\n",
    "\n",
    "# 3. Verify the fix results\n",
    "print(\"Step 3: Verify fix results\")\n",
    "print(f\"NaN count in X_train: {X_train.isna().sum().sum()}\")\n",
    "print(f\"NaN count in X_valid: {X_valid.isna().sum().sum()}\")\n",
    "print(f\"NaN count in X_test: {X_test.isna().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4dfc77",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b93fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Executing Feature Selection...\n",
      "   (Using RandomForestClassifier trained on the validation set to find important features...)\n",
      "âœ… Selected Top 75 features.\n",
      "Top 5 most important features: ['InterestBearingUPB_num_decrease', 'CurrentActualUPB_late_slope', 'InterestBearingUPB_late_std', 'CurrentActualUPB_num_decrease', 'CreditScore']\n",
      "   â†’ Created X_train_selected with shape: (30504, 75)\n",
      "   â†’ Created X_valid_selected with shape: (5370, 75)\n",
      "   â†’ Created X_test_selected with shape: (13426, 75)\n",
      "   â†’ Created X_train_normal_selected (for training) with shape: (30504, 75)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "# Ensure consistent columns before scaling (important!)\n",
    "common_cols = list(set(X_train.columns) & set(X_valid.columns))\n",
    "X_train = X_train[common_cols]\n",
    "X_valid = X_valid[common_cols]\n",
    "X_test = X_test[common_cols]\n",
    "\n",
    "# Drop non-numeric columns AFTER identifying common columns\n",
    "object_cols = X_train.select_dtypes(include=['object']).columns\n",
    "X_train = X_train.drop(columns=object_cols)\n",
    "X_valid = X_valid.drop(columns=object_cols)\n",
    "X_test = X_test.drop(columns=object_cols)\n",
    "\n",
    "# Store feature names BEFORE scaling\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"ðŸš€ Executing Feature Selection...\")\n",
    "print(\"   (Using RandomForestClassifier trained on the validation set to find important features...)\")\n",
    "\n",
    "# Define the selector model\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train the selector on the VALIDATION data (X_valid_scaled, y_valid)\n",
    "# This uses the labels (0s and 1s) in the validation set to determine feature importance\n",
    "rf_selector.fit(X_valid_scaled, y_valid)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_selector.feature_importances_\n",
    "\n",
    "# Check if lengths match (should match if scaler and fit used correctly)\n",
    "if len(feature_names) == len(importances):\n",
    "    # Create importance DataFrame\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    # Select Top N features\n",
    "    TOP_N_FEATURES = 75 # You can adjust this number\n",
    "    top_features = feature_importance_df.head(TOP_N_FEATURES)['feature'].tolist()\n",
    "\n",
    "    print(f\"âœ… Selected Top {TOP_N_FEATURES} features.\")\n",
    "    print(\"Top 5 most important features:\", top_features[:5])\n",
    "\n",
    "    # Create new DataFrames/arrays containing only the selected features\n",
    "    # Need to convert scaled arrays back to DataFrame temporarily to select by name\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "    X_valid_scaled_df = pd.DataFrame(X_valid_scaled, columns=feature_names)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "\n",
    "    # final selected features df\n",
    "    X_train_selected = X_train_scaled_df[top_features]\n",
    "    X_valid_selected = X_valid_scaled_df[top_features]\n",
    "    X_test_selected = X_test_scaled_df[top_features]\n",
    "\n",
    "\n",
    "    # Create the 'normal only' training set for unsupervised models\n",
    "    # Convert y_train to numpy array if it's a pandas Series for boolean indexing\n",
    "    y_train_np = y_train.to_numpy() if isinstance(y_train, pd.Series) else y_train\n",
    "    X_train_normal_selected = X_train_selected[y_train_np == 0]\n",
    "\n",
    "    print(f\"   â†’ Created X_train_selected with shape: {X_train_selected.shape}\")\n",
    "    print(f\"   â†’ Created X_valid_selected with shape: {X_valid_selected.shape}\")\n",
    "    print(f\"   â†’ Created X_test_selected with shape: {X_test_selected.shape}\")\n",
    "    print(f\"   â†’ Created X_train_normal_selected (for training) with shape: {X_train_normal_selected.shape}\")\n",
    "\n",
    "else:\n",
    "    print(\"--- ERROR ---\")\n",
    "    print(\"Length mismatch between feature names and importances.\")\n",
    "    print(f\"Length of feature_names (from X_train before scaling): {len(feature_names)}\")\n",
    "    print(f\"Length of importances (from RandomForest): {len(importances)}\")\n",
    "    print(\"This indicates an issue in data preparation before scaling or in the selector training.\")\n",
    "    print(\"Please check the steps before this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8f2ac",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5f4135f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid positive ratio: 0.1261\n"
     ]
    }
   ],
   "source": [
    "# Print target=1 ratio in valid (concise)\n",
    "print(f\"valid positive ratio: {y_valid.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a4397db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Running Baseline Models (Using Top 75 Selected & Scaled Features)...\n",
      "   â†’ Using 30504 normal training samples for fitting (with 75 features).\n",
      "   â†’ Evaluating on 5370 validation samples (with 75 features).\n",
      "    Fitting IsolationForest...\n",
      "     IsolationForest -> AP: 0.1414, ROC-AUC: 0.5779\n",
      "    Fitting LocalOutlierFactor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sixestates/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LocalOutlierFactor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     LocalOutlierFactor -> AP: 0.2803, ROC-AUC: 0.6743\n",
      "    Fitting PCA_Reconstruction...\n",
      "     PCA_Reconstruction -> AP: 0.1868, ROC-AUC: 0.5884\n",
      "    Fitting OneClassSVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sixestates/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     OneClassSVM -> AP: 0.2061, ROC-AUC: 0.6497\n",
      "\n",
      "âœ… Baseline Results (Top 75 Scaled Features, Trained on Normal Samples Only):\n",
      "                Model      AP  ROC-AUC\n",
      "1  LocalOutlierFactor  0.2803   0.6743\n",
      "3         OneClassSVM  0.2061   0.6497\n",
      "2  PCA_Reconstruction  0.1868   0.5884\n",
      "0     IsolationForest  0.1414   0.5779\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# Baseline Model Evaluation (Using TOP N Selected Features - CORRECTED)\n",
    "# =============================================================\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# No StandardScaler needed here as selected features are already scaled\n",
    "\n",
    "# Check if feature selection variables exist\n",
    "if 'TOP_N_FEATURES' not in locals() or \\\n",
    "   'X_train_selected' not in locals() or \\\n",
    "   'X_valid_selected' not in locals() or \\\n",
    "   'y_train' not in locals() or \\\n",
    "   'y_valid' not in locals():\n",
    "    print(\"--- ERROR ---\")\n",
    "    print(\"Required variables (TOP_N_FEATURES, X_train_selected, X_valid_selected, y_train, y_valid) not found.\")\n",
    "    print(\"Please ensure you have successfully run the 'Feature Selection' cell (id: d5b93fe1) immediately before this cell.\")\n",
    "else:\n",
    "    print(f\"ðŸš€ Running Baseline Models (Using Top {TOP_N_FEATURES} Selected & Scaled Features)...\")\n",
    "\n",
    "    # ---------- Data Preparation ----------\n",
    "    # Use variables from feature selection cell (already scaled)\n",
    "    X_train_ = X_train_selected.copy()\n",
    "    X_valid_ = X_valid_selected.copy()\n",
    "    y_train_ = y_train.copy() # Should contain only 0s\n",
    "    y_valid_ = y_valid.copy() # Contains 0s and 1s\n",
    "\n",
    "    # (===== â­ï¸ Crucial Step: Create the 'normal only' subset for fitting =====)\n",
    "    X_train_fit_normal = X_train_[y_train_ == 0]\n",
    "\n",
    "    print(f\"   â†’ Using {X_train_fit_normal.shape[0]} normal training samples for fitting (with {X_train_fit_normal.shape[1]} features).\")\n",
    "    print(f\"   â†’ Evaluating on {X_valid_.shape[0]} validation samples (with {X_valid_.shape[1]} features).\")\n",
    "\n",
    "    # ---------- Define Models ----------\n",
    "    # Hyperparameters might need re-tuning based on the selected features\n",
    "    models = {\n",
    "        \"IsolationForest\": IsolationForest(\n",
    "            n_estimators=200,\n",
    "            contamination=0.05, # Hyperparameter to tune\n",
    "            random_state=42\n",
    "        ),\n",
    "        \"LocalOutlierFactor\": LocalOutlierFactor(\n",
    "            n_neighbors=10,       # Hyperparameter to tune\n",
    "            novelty=True,\n",
    "            contamination=0.05  # Hyperparameter to tune\n",
    "        ),\n",
    "        \"PCA_Reconstruction\": PCA(\n",
    "            n_components=0.95,\n",
    "            random_state=42     # n_components might need tuning\n",
    "        ),\n",
    "        \"OneClassSVM\": OneClassSVM(\n",
    "            kernel=\"rbf\",\n",
    "            gamma=\"scale\",\n",
    "            nu=0.05             # Hyperparameter to tune\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # ---------- Evaluation Loop ----------\n",
    "    results_selected = [] # Use a distinct list name\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"    Fitting {name}...\")\n",
    "        scores = None\n",
    "        try:\n",
    "            # (===== â­ï¸ Corrected Fit Call =====)\n",
    "            # Fit ALL models ONLY on normal samples\n",
    "            model.fit(X_train_fit_normal)\n",
    "\n",
    "            # (===== â­ï¸ Corrected Evaluation Call =====)\n",
    "            # Evaluate ALL models on the full validation set (X_valid_)\n",
    "            if name == \"PCA_Reconstruction\":\n",
    "                # Ensure X_valid_ is NumPy for calculation if it's a DataFrame\n",
    "                X_valid_np = X_valid_.values if isinstance(X_valid_, pd.DataFrame) else X_valid_\n",
    "                Xv_rec = model.inverse_transform(model.transform(X_valid_np))\n",
    "                scores = np.mean((X_valid_np - Xv_rec) ** 2, axis=1) # Higher error = more anomalous\n",
    "            else: # IF, LOF, OCSVM\n",
    "                # Ensure X_valid_ is compatible with decision_function (DataFrame is usually fine)\n",
    "                scores = -model.decision_function(X_valid_) # Higher score = more anomalous\n",
    "\n",
    "            # --- Calculate Metrics ---\n",
    "            if scores is None or np.any(np.isnan(scores)) or np.any(np.isinf(scores)):\n",
    "                 valid_scores = scores[~(np.isnan(scores) | np.isinf(scores))] if scores is not None else np.array([0.0])\n",
    "                 median_score = np.median(valid_scores) if len(valid_scores) > 0 else 0.0\n",
    "                 scores = np.nan_to_num(scores, nan=median_score, posinf=median_score, neginf=median_score)\n",
    "\n",
    "            if np.sum(y_valid_) == 0 or np.all(y_valid_ == y_valid_[0]):\n",
    "                 ap, roc = np.nan, np.nan\n",
    "            else:\n",
    "                 ap = average_precision_score(y_valid_, scores)\n",
    "                 roc = roc_auc_score(y_valid_, scores)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"     ERROR fitting or predicting {name}: {e}\")\n",
    "            ap, roc = np.nan, np.nan\n",
    "\n",
    "        results_selected.append((name, round(ap, 4), round(roc, 4)))\n",
    "        print(f\"     {name} -> AP: {ap:.4f}, ROC-AUC: {roc:.4f}\")\n",
    "\n",
    "    # ---------- Output Results ----------\n",
    "    results_df_selected = pd.DataFrame(results_selected, columns=[\"Model\", \"AP\", \"ROC-AUC\"]).sort_values(\"AP\", ascending=False)\n",
    "    print(f\"\\nâœ… Baseline Results (Top {X_train_fit_normal.shape[1]} Scaled Features, Trained on Normal Samples Only):\")\n",
    "    print(results_df_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fe5dc6",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "is5126",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
