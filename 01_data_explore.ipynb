{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7d2f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb9cb1",
   "metadata": {},
   "source": [
    "#### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e3947",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_test = pd.read_csv(\"./data/raw_data/loans_test.csv\")\n",
    "loans_train = pd.read_csv(\"./data/raw_data/loans_train.csv\")\n",
    "loans_valid = pd.read_csv(\"./data/raw_data/loans_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1469d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data columns\n",
    "# Correct dataset mapping\n",
    "cols_train = set(loans_train.columns)  # Training set\n",
    "cols_valid = set(loans_valid.columns)  # Validation set\n",
    "cols_test  = set(loans_test.columns)   # Test set\n",
    "\n",
    "common_cols = cols_train & cols_valid & cols_test\n",
    "only_train  = cols_train - common_cols\n",
    "only_valid  = cols_valid - common_cols\n",
    "only_test   = cols_test  - common_cols\n",
    "uncommon    = (cols_train | cols_valid | cols_test) - common_cols\n",
    "\n",
    "print(\"Number of common columns:\", len(common_cols))\n",
    "print(\"Common columns example:\", sorted(list(common_cols))[:10], \"...\")\n",
    "print(\"\\nColumns only in train:\", sorted(list(only_train)))\n",
    "print(\"Columns only in valid:\", sorted(list(only_valid)))\n",
    "print(\"Columns only in test:\", sorted(list(only_test)))\n",
    "print(\"\\nAll uncommon columns:\", sorted(list(uncommon)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e300e",
   "metadata": {},
   "source": [
    "### Data type check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime format\n",
    "\n",
    "# Convert FirstPaymentDate and MaturityDate\n",
    "for col in ['FirstPaymentDate', 'MaturityDate']:\n",
    "    if col in loans_train.columns:\n",
    "        loans_train[col] = pd.to_datetime(loans_train[col])\n",
    "        loans_valid[col] = pd.to_datetime(loans_valid[col])\n",
    "        loans_test[col] = pd.to_datetime(loans_test[col])\n",
    "\n",
    "# Convert MonthlyReportingPeriod columns (YYYYMM format)\n",
    "monthly_cols = [col for col in loans_train.columns if 'MonthlyReportingPeriod' in col]\n",
    "for col in monthly_cols:\n",
    "    # Convert YYYYMM format to datetime\n",
    "    loans_train[col] = pd.to_datetime(loans_train[col], format='%Y%m')\n",
    "    loans_valid[col] = pd.to_datetime(loans_valid[col], format='%Y%m')\n",
    "    loans_test[col] = pd.to_datetime(loans_test[col], format='%Y%m')\n",
    "\n",
    "print('converted columns: FirstPaymentDate, MaturityDate, MonthlyReportingPeriod')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b20ed51",
   "metadata": {},
   "source": [
    "### Encode for Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all object columns sorted by unique values (ascending)\n",
    "print(\"=== All Object Columns (Sorted by Unique Values) ===\")\n",
    "object_cols = loans_train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Found {len(object_cols)} object columns:\")\n",
    "print()\n",
    "\n",
    "# Create list of columns with their unique counts for sorting\n",
    "col_info = []\n",
    "for col in object_cols:\n",
    "    unique_count = loans_train[col].nunique()\n",
    "    col_info.append((col, unique_count))\n",
    "\n",
    "# Sort by unique values (ascending)\n",
    "col_info.sort(key=lambda x: x[1])\n",
    "\n",
    "for i, (col, unique_count) in enumerate(col_info, 1):\n",
    "    print(f\"{i}. {col}\")\n",
    "    print(f\"   Data type: {loans_train[col].dtype}\")\n",
    "    print(f\"   Unique values: {unique_count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding for categorical columns\n",
    "print(\"=== One-Hot Encoding for Categorical Columns ===\")\n",
    "\n",
    "# Columns to be one-hot encoded (based on unique values analysis)\n",
    "one_hot_columns = [\n",
    "    'PPM_Flag', 'ProductType', 'SuperConformingFlag', 'InterestOnlyFlag',\n",
    "    'FirstTimeHomebuyerFlag', 'OccupancyStatus', 'Channel', 'LoanPurpose',\n",
    "    'ProgramIndicator', 'BalloonIndicator', 'PropertyType'\n",
    "]\n",
    "\n",
    "print(f\"Columns to be one-hot encoded: {len(one_hot_columns)}\")\n",
    "print(\"Columns:\", one_hot_columns)\n",
    "print()\n",
    "\n",
    "# Record original shapes\n",
    "original_train_shape = loans_train.shape\n",
    "original_valid_shape = loans_valid.shape    \n",
    "original_test_shape = loans_test.shape\n",
    "\n",
    "# Apply one-hot encoding (directly overwrite original DataFrames)\n",
    "for col in one_hot_columns:\n",
    "    if col in loans_train.columns:\n",
    "        print(f\"Processing {col}...\")\n",
    "        \n",
    "        # Get all unique values from all datasets (excluding NaN)\n",
    "        all_values = set()\n",
    "        for df in [loans_train, loans_valid, loans_test]:\n",
    "            unique_vals = df[col].dropna().unique()\n",
    "            all_values.update(unique_vals)\n",
    "        all_values = sorted(list(all_values))\n",
    "        \n",
    "        print(f\"  Unique values: {all_values}\")\n",
    "        \n",
    "        # Create one-hot encoded columns\n",
    "        for value in all_values:\n",
    "            new_col_name = f\"{col}_{value}\"\n",
    "            loans_train[new_col_name] = (loans_train[col] == value).astype(int)\n",
    "            loans_valid[new_col_name] = (loans_valid[col] == value).astype(int)\n",
    "            loans_test[new_col_name] = (loans_test[col] == value).astype(int)\n",
    "        \n",
    "        # Drop original column\n",
    "        loans_train.drop(columns=[col], inplace=True)\n",
    "        loans_valid.drop(columns=[col], inplace=True)\n",
    "        loans_test.drop(columns=[col], inplace=True)\n",
    "        \n",
    "        print(f\"  Created {len(all_values)} one-hot columns\")\n",
    "        print()\n",
    "\n",
    "print(\"=== One-Hot Encoding Complete ===\")\n",
    "print(f\"Original shape - Train: {original_train_shape}, Valid: {original_valid_shape}, Test: {original_test_shape}\")\n",
    "print(f\"New shape - Train: {loans_train.shape}, Valid: {loans_valid.shape}, Test: {loans_test.shape}\")\n",
    "print(f\"New features added: {loans_train.shape[1] - original_train_shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be23e43",
   "metadata": {},
   "source": [
    "### Data vacancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e174658",
   "metadata": {},
   "source": [
    "***Drop columns with 100% missing data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079325d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive analysis of data vacancy\n",
    "\n",
    "# 1. Overall missing data summary\n",
    "missing_summary = loans_train.isnull().sum().sort_values(ascending=False)\n",
    "missing_pct = (missing_summary / len(loans_train)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_summary,\n",
    "    'Missing_Percentage': missing_pct\n",
    "})\n",
    "\n",
    "print(f\"Total features: {len(loans_train.columns)}\")\n",
    "print(f\"Features with missing data: {(missing_summary > 0).sum()}\")\n",
    "print(f\"Features with complete data: {(missing_summary == 0).sum()}\")\n",
    "\n",
    "# Show top 20 features with most missing data\n",
    "print(\"Top 20 features with most missing data:\")\n",
    "print(missing_df.head(20).to_string())\n",
    "\n",
    "# 2. Missing data patterns by feature type\n",
    "# Categorize features\n",
    "static_features = [col for col in loans_train.columns if not col.startswith(('0_', '1_', '2_', '3_', '4_', '5_', '6_', '7_', '8_', '9_', '10_', '11_', '12_', '13_')) and col not in ['index', 'target']]\n",
    "time_series_features = [col for col in loans_train.columns if col.startswith(('0_', '1_', '2_', '3_', '4_', '5_', '6_', '7_', '8_', '9_', '10_', '11_', '12_', '13_'))]\n",
    "\n",
    "print(f\"Static features: {len(static_features)}\")\n",
    "print(f\"Time series features: {len(time_series_features)}\")\n",
    "print()\n",
    "\n",
    "# Analyze missing data by feature type\n",
    "static_missing = loans_train[static_features].isnull().sum()\n",
    "time_series_missing = loans_train[time_series_features].isnull().sum()\n",
    "\n",
    "print(\"Static features missing data summary:\")\n",
    "print(f\"  Features with missing data: {(static_missing > 0).sum()}\")\n",
    "print(f\"  Average missing percentage: {static_missing.mean() / len(loans_train) * 100:.2f}%\")\n",
    "\n",
    "print(\"Time series features missing data summary:\")\n",
    "print(f\"  Features with missing data: {(time_series_missing > 0).sum()}\")\n",
    "print(f\"  Average missing percentage: {time_series_missing.mean() / len(loans_train) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7477f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with 100% missing data directly\n",
    "columns_to_drop = ['ReliefRefinanceIndicator', 'PreHARP_Flag']\n",
    "\n",
    "# Drop columns directly from original datasets\n",
    "loans_train.drop(columns=columns_to_drop, inplace=True)\n",
    "loans_valid.drop(columns=columns_to_drop, inplace=True)\n",
    "loans_test.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c534ca2",
   "metadata": {},
   "source": [
    "***process missing data(999)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a6c7b5",
   "metadata": {},
   "source": [
    "CreditScore: Values outside range or missing coded as 9999.\n",
    "\n",
    "MI_Pct: 999 = not available.\n",
    "\n",
    "OriginalDTI: Values > 65% or missing coded as 999.\n",
    "\n",
    "OriginalLTV: Invalid coded as 999.\n",
    "\n",
    "N_EstimatedLTV: Range 1–998, with 999 = unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c8fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process missing data (999) - Complete processing for all 999/9999 values\n",
    "\n",
    "# Define static columns with 999 as missing value indicator\n",
    "static_columns_with_999 = [\n",
    "    'CreditScore',    # Values outside range or missing coded as 9999\n",
    "    'MI_Pct',         # 999 = not available\n",
    "    'OriginalDTI',    # Values > 65% or missing coded as 999\n",
    "    'OriginalLTV',    # Invalid coded as 999\n",
    "]\n",
    "\n",
    "# Find time series columns with 999 as missing value indicator\n",
    "# N_EstimatedLTV is a time series variable (N = month index: 0, 1, 2, ...)\n",
    "time_series_columns_with_999 = [col for col in loans_train.columns if 'EstimatedLTV' in col]\n",
    "\n",
    "print(\"1. Static columns with 999/9999 as missing value indicator:\")\n",
    "for col in static_columns_with_999:\n",
    "    if col in loans_train.columns:\n",
    "        count_999 = (loans_train[col] == 999).sum()\n",
    "        count_9999 = (loans_train[col] == 9999).sum() if col == 'CreditScore' else 0\n",
    "        total_missing = count_999 + count_9999\n",
    "        print(f\"- {col}: {total_missing} missing values (999: {count_999}, 9999: {count_9999})\")\n",
    "    else:\n",
    "        print(f\"- {col}: Column not found in dataset\")\n",
    "print()\n",
    "\n",
    "print(f\"2. Time series columns with 999 as missing value indicator ({len(time_series_columns_with_999)} found):\")\n",
    "for col in time_series_columns_with_999:\n",
    "    count_999 = (loans_train[col] == 999).sum()\n",
    "    print(f\"   - {col}: {count_999} missing values (999)\")\n",
    "print()\n",
    "\n",
    "# Process static columns\n",
    "print(\"3. Processing static columns:\")\n",
    "for col in static_columns_with_999:\n",
    "    if col in loans_train.columns:\n",
    "        # Replace 999 values with NaN\n",
    "        loans_train[col] = loans_train[col].replace(999, np.nan)\n",
    "        loans_valid[col] = loans_valid[col].replace(999, np.nan)\n",
    "        loans_test[col] = loans_test[col].replace(999, np.nan)\n",
    "        \n",
    "        # For CreditScore, also replace 9999 values\n",
    "        if col == 'CreditScore':\n",
    "            loans_train[col] = loans_train[col].replace(9999, np.nan)\n",
    "            loans_valid[col] = loans_valid[col].replace(9999, np.nan)\n",
    "            loans_test[col] = loans_test[col].replace(9999, np.nan)\n",
    "        \n",
    "        print(f\"   ✓ Processed {col}: Replaced 999/9999 values with NaN\")\n",
    "\n",
    "# Process time series columns\n",
    "print(\"\\n4. Processing time series columns:\")\n",
    "if len(time_series_columns_with_999) > 0:\n",
    "    for col in time_series_columns_with_999:\n",
    "        loans_train[col] = loans_train[col].replace(999, np.nan)\n",
    "        loans_valid[col] = loans_valid[col].replace(999, np.nan)\n",
    "        loans_test[col] = loans_test[col].replace(999, np.nan)\n",
    "        print(f\" ✓ Processed {col}: Replaced 999 values with NaN\")\n",
    "else:\n",
    "    print(\"   - No time series EstimatedLTV columns found in dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e37b4f",
   "metadata": {},
   "source": [
    "### Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a8271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution Analysis - Non-Object Type Columns\n",
    "\n",
    "columns_to_drop = ['index']  # Add columns to exclude from analysis\n",
    "\n",
    "# Get all columns\n",
    "all_cols = loans_train.columns.tolist()\n",
    "\n",
    "# Select only non-object type columns (numerical, datetime, etc.)\n",
    "non_object_cols = loans_train.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Remove columns in the drop list\n",
    "non_object_cols = [col for col in non_object_cols if col not in columns_to_drop]\n",
    "\n",
    "print(f\"- Total columns: {len(all_cols)}\")\n",
    "print(f\"- Non-object type columns: {len(non_object_cols)}\")\n",
    "print(f\"- Columns dropped from analysis: {columns_to_drop}\")\n",
    "print(f\"- Object type columns (excluded): {len(all_cols) - len(non_object_cols)}\")\n",
    "\n",
    "# Group by data type\n",
    "data_types = {}\n",
    "for col in non_object_cols:\n",
    "    dtype = str(loans_train[col].dtype)\n",
    "    if dtype not in data_types:\n",
    "        data_types[dtype] = []\n",
    "    data_types[dtype].append(col)\n",
    "\n",
    "\n",
    "# 3. Data range analysis for numerical columns\n",
    "numerical_cols = loans_train[non_object_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "if len(numerical_cols) > 0:\n",
    "    print(\"3. Data Range Analysis for Numerical Columns:\")\n",
    "    numerical_stats = loans_train[numerical_cols].describe()\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        min_val = numerical_stats.loc['min', col]\n",
    "        max_val = numerical_stats.loc['max', col]\n",
    "        mean_val = numerical_stats.loc['mean', col]\n",
    "        std_val = numerical_stats.loc['std', col]\n",
    "        \n",
    "        print(f\"- {col}:\")\n",
    "        print(f\"  Range: [{min_val:.2f}, {max_val:.2f}]\")\n",
    "        print(f\"  Mean: {mean_val:.2f}, Std: {std_val:.2f}\")\n",
    "    print()\n",
    "\n",
    "# 4. Distribution histograms for numerical columns\n",
    "if len(numerical_cols) > 0:\n",
    "    print(\"4. Distribution Histograms for Numerical Columns:\")\n",
    "    \n",
    "    # Calculate subplot layout\n",
    "    n_cols = len(numerical_cols)\n",
    "    n_rows = (n_cols + 2) // 3  # 3 columns per row\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        row = i // 3\n",
    "        col_idx = i % 3\n",
    "        \n",
    "        # Create histogram\n",
    "        axes[row, col_idx].hist(loans_train[col].dropna(), bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[row, col_idx].set_title(f'{col}\\nRange: [{loans_train[col].min():.2f}, {loans_train[col].max():.2f}]')\n",
    "        axes[row, col_idx].set_xlabel(col)\n",
    "        axes[row, col_idx].set_ylabel('Frequency')\n",
    "        axes[row, col_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_cols, n_rows * 3):\n",
    "        row = i // 3\n",
    "        col_idx = i % 3\n",
    "        axes[row, col_idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No numerical columns found.\")\n",
    "\n",
    "# 5. Summary statistics\n",
    "print(\"5. Summary Statistics:\")\n",
    "print(f\"- Total non-object columns analyzed: {len(non_object_cols)}\")\n",
    "print(f\"- Numerical columns: {len(numerical_cols)}\")\n",
    "\n",
    "# Show basic statistics for numerical columns\n",
    "if len(numerical_cols) > 0:\n",
    "    print(\"\\nBasic Statistics for Numerical Columns:\")\n",
    "    print(loans_train[numerical_cols].describe().round(2))\n",
    "else:\n",
    "    print(\"- No numerical columns found for analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
