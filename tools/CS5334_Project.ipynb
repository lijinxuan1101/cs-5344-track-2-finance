{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a134b32",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7d2f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb9cb1",
   "metadata": {},
   "source": [
    "***Load datasets and explore***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e3947",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_test = pd.read_csv(\"./data/raw_data/loans_test.csv\")\n",
    "loans_train = pd.read_csv(\"./data/raw_data/loans_train.csv\")\n",
    "loans_valid = pd.read_csv(\"./data/raw_data/loans_valid.csv\")\n",
    "\n",
    "# check data columns\n",
    "# Correct dataset mapping\n",
    "cols_train = set(loans_train.columns)  # Training set\n",
    "cols_valid = set(loans_valid.columns)  # Validation set\n",
    "cols_test  = set(loans_test.columns)   # Test set\n",
    "\n",
    "common_cols = cols_train & cols_valid & cols_test\n",
    "only_train  = cols_train - common_cols\n",
    "only_valid  = cols_valid - common_cols\n",
    "only_test   = cols_test  - common_cols\n",
    "uncommon    = (cols_train | cols_valid | cols_test) - common_cols\n",
    "\n",
    "print(\"Number of common columns:\", len(common_cols))\n",
    "print(\"Common columns example:\", sorted(list(common_cols))[:10], \"...\")\n",
    "print(\"\\nColumns only in train:\", sorted(list(only_train)))\n",
    "print(\"Columns only in valid:\", sorted(list(only_valid)))\n",
    "print(\"Columns only in test:\", sorted(list(only_test)))\n",
    "print(\"\\nAll uncommon columns:\", sorted(list(uncommon)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ac5905",
   "metadata": {},
   "source": [
    "- train set(target=0)\n",
    "- valid,test set(target=0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043d839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0aa97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Display Data Types for All Datasets ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display data types for each dataset\n",
    "# datasets = {'Train': loans_train, 'Valid': loans_valid, 'Test': loans_test}\n",
    "datasets = {'Train': loans_train}\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n=== {name} Dataset ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    # Show data types summary\n",
    "    print(f\"\\nData types summary:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    # Show detailed column information\n",
    "    print(f\"\\nAll columns with data types:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, (col, dtype) in enumerate(df.dtypes.items(), 1):\n",
    "        non_null_count = df[col].count()\n",
    "        null_count = len(df) - non_null_count\n",
    "        print(f\"{i:3d}. {col:<50} | {str(dtype):<15} | Non-null: {non_null_count:5d} | Null: {null_count:5d}\")\n",
    "    \n",
    "    # Check for problematic data types\n",
    "    object_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    datetime_cols = df.select_dtypes(include=['datetime64']).columns\n",
    "    \n",
    "    if len(object_cols) > 0:\n",
    "        print(f\"\\nâš ï¸  Object/Categorical columns ({len(object_cols)}):\")\n",
    "        for col in object_cols:\n",
    "            print(f\"  - {col}: {df[col].dtype}\")\n",
    "    \n",
    "    if len(datetime_cols) > 0:\n",
    "        print(f\"\\nâš ï¸  Datetime columns ({len(datetime_cols)}):\")\n",
    "        for col in datetime_cols:\n",
    "            print(f\"  - {col}: {df[col].dtype}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nâœ… Data types analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e300e",
   "metadata": {},
   "source": [
    "***Data type check***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime format\n",
    "\n",
    "# Convert FirstPaymentDate and MaturityDate\n",
    "for col in ['FirstPaymentDate', 'MaturityDate']:\n",
    "    if col in loans_train.columns:\n",
    "        loans_train[col] = pd.to_datetime(loans_train[col])\n",
    "        loans_valid[col] = pd.to_datetime(loans_valid[col])\n",
    "        loans_test[col] = pd.to_datetime(loans_test[col])\n",
    "\n",
    "# Convert MonthlyReportingPeriod columns (YYYYMM format)\n",
    "monthly_cols = [col for col in loans_train.columns if 'MonthlyReportingPeriod' in col]\n",
    "for col in monthly_cols:\n",
    "    # Convert YYYYMM format to datetime\n",
    "    loans_train[col] = pd.to_datetime(loans_train[col], format='%Y%m')\n",
    "    loans_valid[col] = pd.to_datetime(loans_valid[col], format='%Y%m')\n",
    "    loans_test[col] = pd.to_datetime(loans_test[col], format='%Y%m')\n",
    "\n",
    "print('converted columns: FirstPaymentDate, MaturityDate, MonthlyReportingPeriod')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e174658",
   "metadata": {},
   "source": [
    "***Drop columns with 100% missing data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079325d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive analysis of data vacancy\n",
    "\n",
    "# 1. Overall missing data summary\n",
    "missing_summary = loans_train.isnull().sum().sort_values(ascending=False)\n",
    "missing_pct = (missing_summary / len(loans_train)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_summary,\n",
    "    'Missing_Percentage': missing_pct\n",
    "})\n",
    "\n",
    "print(f\"Total features: {len(loans_train.columns)}\")\n",
    "print(f\"Features with missing data: {(missing_summary > 0).sum()}\")\n",
    "print(f\"Features with complete data: {(missing_summary == 0).sum()}\")\n",
    "\n",
    "# Show top 20 features with most missing data\n",
    "print(\"Top 20 features with most missing data:\")\n",
    "print(missing_df.head(5).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7477f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with 100% missing data directly\n",
    "columns_to_drop = ['ReliefRefinanceIndicator', 'PreHARP_Flag']\n",
    "\n",
    "# Drop columns directly from original datasets\n",
    "loans_train.drop(columns=columns_to_drop, inplace=True)\n",
    "loans_valid.drop(columns=columns_to_drop, inplace=True)\n",
    "loans_test.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c534ca2",
   "metadata": {},
   "source": [
    "***process missing data(999)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a6c7b5",
   "metadata": {},
   "source": [
    " - CreditScore: Values outside range or missing coded as 9999.\n",
    " - MI_Pct: 999 = not available.\n",
    " - OriginalDTI: Values > 65% or missing coded as 999.\n",
    " - OriginalLTV: Invalid coded as 999.\n",
    " - N_EstimatedLTV: Range 1â€“998, with 999 = unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CreditScore\n",
    "\n",
    "def fix_credit_score_precision(df):\n",
    "    # Step 1: clean invalid codes\n",
    "    df['CreditScore'] = df['CreditScore'].replace({9999: np.nan})\n",
    "    df.loc[(df['CreditScore'] < 300) | (df['CreditScore'] > 850), 'CreditScore'] = np.nan\n",
    "\n",
    "    # Step 2: create a missing indicator (important for precision)\n",
    "    df['CreditScore_missing'] = df['CreditScore'].isna().astype(int)\n",
    "\n",
    "    # Step 3: grouped median imputation\n",
    "    if 'LoanPurpose' in df.columns:\n",
    "        df['CreditScore'] = df.groupby('LoanPurpose')['CreditScore'].transform(\n",
    "            lambda x: x.fillna(x.median())\n",
    "        )\n",
    "    else:\n",
    "        df['CreditScore'] = df['CreditScore'].fillna(df['CreditScore'].median())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply\n",
    "loans_train = fix_credit_score_precision(loans_train)\n",
    "loans_valid = fix_credit_score_precision(loans_valid)\n",
    "loans_test  = fix_credit_score_precision(loans_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebe4f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI_Pct\n",
    "\n",
    "def fix_mi_pct_precision(df):\n",
    "    # 1. Clean invalid codes\n",
    "    df['MI_Pct'] = df['MI_Pct'].replace(999, np.nan)\n",
    "\n",
    "    # 2. Add missing indicator (important for precision)\n",
    "    df['MI_Pct_missing'] = df['MI_Pct'].isna().astype(int)\n",
    "\n",
    "    # 3. Grouped median imputation â€” use correlated features if available\n",
    "    group_cols = [col for col in ['PropertyState', 'LoanPurpose', 'ServicerName'] if col in df.columns]\n",
    "    if group_cols:\n",
    "        df['MI_Pct'] = df.groupby(group_cols)['MI_Pct'].transform(lambda x: x.fillna(x.median()))\n",
    "    else:\n",
    "        df['MI_Pct'] = df['MI_Pct'].fillna(df['MI_Pct'].median())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply\n",
    "loans_train = fix_mi_pct_precision(loans_train)\n",
    "loans_valid = fix_mi_pct_precision(loans_valid)\n",
    "loans_test  = fix_mi_pct_precision(loans_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c582f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OriginalDTI\n",
    "\n",
    "def fix_dti_precision(df):\n",
    "    # 1. Replace invalid values with NaN\n",
    "    df.loc[df['OriginalDTI'] > 65, 'OriginalDTI'] = np.nan\n",
    "    df['OriginalDTI'] = df['OriginalDTI'].replace(999, np.nan)\n",
    "\n",
    "    # 2. Add missing indicator\n",
    "    df['OriginalDTI_missing'] = df['OriginalDTI'].isna().astype(int)\n",
    "\n",
    "    # 3. Grouped median imputation â€” keep contextual patterns\n",
    "    group_cols = [col for col in ['LoanPurpose', 'PropertyState'] if col in df.columns]\n",
    "    if group_cols:\n",
    "        df['OriginalDTI'] = df.groupby(group_cols)['OriginalDTI'].transform(lambda x: x.fillna(x.median()))\n",
    "    else:\n",
    "        df['OriginalDTI'] = df['OriginalDTI'].fillna(df['OriginalDTI'].median())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply\n",
    "loans_train = fix_dti_precision(loans_train)\n",
    "loans_valid = fix_dti_precision(loans_valid)\n",
    "loans_test  = fix_dti_precision(loans_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfdf261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OriginalLTV\n",
    "\n",
    "def fix_ltv_precision(df):\n",
    "    # 1. Replace invalid values\n",
    "    df['OriginalLTV'] = df['OriginalLTV'].replace(999, np.nan)\n",
    "    df.loc[df['OriginalLTV'] > 100, 'OriginalLTV'] = np.nan\n",
    "\n",
    "    # 2. Add missing flag\n",
    "    df['OriginalLTV_missing'] = df['OriginalLTV'].isna().astype(int)\n",
    "\n",
    "    # 3. Group-based median imputation\n",
    "    group_cols = [col for col in ['PropertyState', 'LoanPurpose', 'ServicerName'] if col in df.columns]\n",
    "    if group_cols:\n",
    "        df['OriginalLTV'] = df.groupby(group_cols)['OriginalLTV'].transform(lambda x: x.fillna(x.median()))\n",
    "    else:\n",
    "        df['OriginalLTV'] = df['OriginalLTV'].fillna(df['OriginalLTV'].median())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply\n",
    "loans_train = fix_ltv_precision(loans_train)\n",
    "loans_valid = fix_ltv_precision(loans_valid)\n",
    "loans_test  = fix_ltv_precision(loans_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ea2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_EstimatedLTV\n",
    "\n",
    "def fix_estimated_ltv_precision(df, time_points=14):\n",
    "    \"\"\"\n",
    "    Precision-oriented cleaning for N_EstimatedLTV panel features.\n",
    "    \"\"\"\n",
    "    for t in range(time_points):\n",
    "        col = f\"{t}_EstimatedLTV\"\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # 1. Replace invalid values\n",
    "        df[col] = df[col].replace(999, np.nan)\n",
    "        df.loc[df[col] > 150, col] = np.nan\n",
    "\n",
    "        # 2. Create missing indicator\n",
    "        df[f\"{col}_missing\"] = df[col].isna().astype(int)\n",
    "\n",
    "    # 3. Row-wise imputation: fill each loanâ€™s missing months\n",
    "    time_cols = [f\"{t}_EstimatedLTV\" for t in range(time_points) if f\"{t}_EstimatedLTV\" in df.columns]\n",
    "    df[time_cols] = df[time_cols].apply(\n",
    "        lambda row: row.fillna(row.median()), axis=1\n",
    "    )\n",
    "\n",
    "    # 4. If still NaN (all months missing), fill with global median\n",
    "    df[time_cols] = df[time_cols].fillna(df[time_cols].median())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply\n",
    "loans_train = fix_estimated_ltv_precision(loans_train)\n",
    "loans_valid = fix_estimated_ltv_precision(loans_valid)\n",
    "loans_test  = fix_estimated_ltv_precision(loans_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e37b4f",
   "metadata": {},
   "source": [
    "***Distribution Analysis***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a8271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution Analysis - Non-Object Type Columns\n",
    "\n",
    "columns_to_drop = ['index']  # Add columns to exclude from analysis\n",
    "\n",
    "# Get all columns\n",
    "all_cols = loans_train.columns.tolist()\n",
    "\n",
    "# Select only non-object type columns (numerical, datetime, etc.)\n",
    "non_object_cols = loans_train.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Remove columns in the drop list\n",
    "non_object_cols = [col for col in non_object_cols if col not in columns_to_drop]\n",
    "\n",
    "print(f\"- Total columns: {len(all_cols)}\")\n",
    "print(f\"- Non-object type columns: {len(non_object_cols)}\")\n",
    "print(f\"- Columns dropped from analysis: {columns_to_drop}\")\n",
    "print(f\"- Object type columns (excluded): {len(all_cols) - len(non_object_cols)}\")\n",
    "\n",
    "# Group by data type\n",
    "data_types = {}\n",
    "for col in non_object_cols:\n",
    "    dtype = str(loans_train[col].dtype)\n",
    "    if dtype not in data_types:\n",
    "        data_types[dtype] = []\n",
    "    data_types[dtype].append(col)\n",
    "\n",
    "\n",
    "# 3. Data range analysis for numerical columns\n",
    "numerical_cols = loans_train[non_object_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "if len(numerical_cols) > 0:\n",
    "    print(\"3. Data Range Analysis for Numerical Columns:\")\n",
    "    numerical_stats = loans_train[numerical_cols].describe()\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        min_val = numerical_stats.loc['min', col]\n",
    "        max_val = numerical_stats.loc['max', col]\n",
    "        mean_val = numerical_stats.loc['mean', col]\n",
    "        std_val = numerical_stats.loc['std', col]\n",
    "        \n",
    "        print(f\"- {col}:\")\n",
    "        print(f\"  Range: [{min_val:.2f}, {max_val:.2f}]\")\n",
    "        print(f\"  Mean: {mean_val:.2f}, Std: {std_val:.2f}\")\n",
    "    print()\n",
    "\n",
    "# 4. Distribution histograms for numerical columns\n",
    "if len(numerical_cols) > 0:\n",
    "    print(\"4. Distribution Histograms for Numerical Columns:\")\n",
    "    \n",
    "    # Calculate subplot layout\n",
    "    n_cols = len(numerical_cols)\n",
    "    n_rows = (n_cols + 2) // 3  # 3 columns per row\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        row = i // 3\n",
    "        col_idx = i % 3\n",
    "        \n",
    "        # Create histogram\n",
    "        axes[row, col_idx].hist(loans_train[col].dropna(), bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[row, col_idx].set_title(f'{col}\\nRange: [{loans_train[col].min():.2f}, {loans_train[col].max():.2f}]')\n",
    "        axes[row, col_idx].set_xlabel(col)\n",
    "        axes[row, col_idx].set_ylabel('Frequency')\n",
    "        axes[row, col_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_cols, n_rows * 3):\n",
    "        row = i // 3\n",
    "        col_idx = i % 3\n",
    "        axes[row, col_idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No numerical columns found.\")\n",
    "\n",
    "# 5. Summary statistics\n",
    "print(\"5. Summary Statistics:\")\n",
    "print(f\"- Total non-object columns analyzed: {len(non_object_cols)}\")\n",
    "print(f\"- Numerical columns: {len(numerical_cols)}\")\n",
    "\n",
    "# Show basic statistics for numerical columns\n",
    "if len(numerical_cols) > 0:\n",
    "    print(\"\\nBasic Statistics for Numerical Columns:\")\n",
    "    print(loans_train[numerical_cols].describe().round(2))\n",
    "else:\n",
    "    print(\"- No numerical columns found for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a71156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output loans_train static columns and panel columns\n",
    "\n",
    "print(\"=== loans_train Column Structure Analysis ===\")\n",
    "print(f\"Total columns: {len(loans_train.columns)}\")\n",
    "\n",
    "# Static features (non-time series features)\n",
    "static_features = [col for col in loans_train.columns \n",
    "                   if not col.startswith(('0_', '1_', '2_', '3_', '4_', '5_', '6_', '7_', '8_', '9_', '10_', '11_', '12_', '13_')) \n",
    "                   and col not in ['index', 'target']]\n",
    "\n",
    "# Time series features (panel features)\n",
    "panel_features = [col for col in loans_train.columns \n",
    "                  if col.startswith(('0_', '1_', '2_', '3_', '4_', '5_', '6_', '7_', '8_', '9_', '10_', '11_', '12_', '13_'))]\n",
    "\n",
    "# Special columns\n",
    "special_cols = ['index', 'target']\n",
    "\n",
    "print(\"1. Static Features:\")\n",
    "print(f\"   Count: {len(static_features)}\")\n",
    "print(\"   Column names:\")\n",
    "for i, col in enumerate(sorted(static_features), 1):\n",
    "    print(f\"   {i:2d}. {col}\")\n",
    "\n",
    "\n",
    "print(\"2. Time Series Features (Panel Features):\")\n",
    "print(f\"   Count: {len(panel_features)}\")\n",
    "print(\"   Grouped by time points:\")\n",
    "panel_dict = {}\n",
    "for col in sorted(panel_features):\n",
    "    time_point = col.split('_')[0]\n",
    "    if time_point not in panel_dict:\n",
    "        panel_dict[time_point] = []\n",
    "    panel_dict[time_point].append(col)\n",
    "\n",
    "for time_point in sorted(panel_dict.keys(), key=lambda x: int(x)):\n",
    "    print(f\"   Time point {time_point}:\")\n",
    "    for col in sorted(panel_dict[time_point]):\n",
    "        feature_name = col.split('_', 1)[1]  # Get feature name\n",
    "        print(f\"     - {feature_name}\")\n",
    "\n",
    "print(\"3. Special Columns:\")\n",
    "print(f\"   Count: {len(special_cols)}\")\n",
    "for i, col in enumerate(special_cols, 1):\n",
    "    print(f\"   {i}. {col}\")\n",
    "\n",
    "print(\"4. Summary Statistics:\")\n",
    "print(f\"   - Static features: {len(static_features)} columns\")\n",
    "print(f\"   - Time series features: {len(panel_features)} columns\")\n",
    "print(f\"   - Special columns: {len(special_cols)} columns\")\n",
    "print(f\"   - Total: {len(static_features) + len(panel_features) + len(special_cols)} columns\")\n",
    "print(f\"   - Time points: {len(panel_dict)} (0-{max([int(k) for k in panel_dict.keys()])})\")\n",
    "print(f\"   - Features per time point: {len(panel_features) // len(panel_dict)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f2b21",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164d051a",
   "metadata": {},
   "source": [
    "***One-hot encode for Object***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all object columns sorted by unique values (ascending)\n",
    "print(\"=== All Object Columns (Sorted by Unique Values) ===\")\n",
    "object_cols = loans_train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Found {len(object_cols)} object columns:\")\n",
    "print()\n",
    "\n",
    "# Create list of columns with their unique counts for sorting\n",
    "col_info = []\n",
    "for col in object_cols:\n",
    "    unique_count = loans_train[col].nunique()\n",
    "    col_info.append((col, unique_count))\n",
    "\n",
    "# Sort by unique values (ascending)\n",
    "col_info.sort(key=lambda x: x[1])\n",
    "\n",
    "for i, (col, unique_count) in enumerate(col_info, 1):\n",
    "    print(f\"{i}. {col}\")\n",
    "    print(f\"   Data type: {loans_train[col].dtype}\")\n",
    "    print(f\"   Unique values: {unique_count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# One-Hot Encoding for categorical columns (sklearn, single-DataFrame API)\n",
    "class SklearnOHE:\n",
    "    def __init__(self, categorical_cols):\n",
    "        self.categorical_cols = list(categorical_cols)\n",
    "        self.encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=np.int8)\n",
    "        self.feature_names_ = None\n",
    "        self.existing_cols_ = None\n",
    "        self.fitted_ = False\n",
    "\n",
    "    def fit(self, df):\n",
    "        # Keep only columns that exist in df\n",
    "        self.existing_cols_ = [c for c in self.categorical_cols if c in df.columns]\n",
    "        X_cat = df[self.existing_cols_].astype('object')\n",
    "        self.encoder.fit(X_cat)\n",
    "        self.feature_names_ = self.encoder.get_feature_names_out(self.existing_cols_)\n",
    "        self.fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        assert self.fitted_, \"Encoder is not fitted. Call fit(df) first.\"\n",
    "        X_cat = df[self.existing_cols_].astype('object')\n",
    "        ohe = pd.DataFrame(self.encoder.transform(X_cat), columns=self.feature_names_, index=df.index)\n",
    "        df_out = df.drop(columns=self.existing_cols_)\n",
    "        df_out = pd.concat([df_out, ohe], axis=1)\n",
    "        return df_out\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "        return self.fit(df).transform(df)\n",
    "\n",
    "# Specify columns to encode\n",
    "one_hot_columns = [\n",
    "    'PPM_Flag', 'ProductType', 'SuperConformingFlag', 'InterestOnlyFlag', 'FirstTimeHomebuyerFlag', 'OccupancyStatus', 'Channel', 'LoanPurpose', 'ProgramIndicator', 'BalloonIndicator', 'PropertyType'\n",
    "]\n",
    "\n",
    "print(\"=== One-Hot Encoding for Categorical Columns (sklearn) ===\")\n",
    "ohe = SklearnOHE(one_hot_columns)\n",
    "# Call three times: train (fit), valid (transform), test (transform)\n",
    "loans_train = ohe.fit_transform(loans_train)\n",
    "loans_valid = ohe.transform(loans_valid)\n",
    "loans_test = ohe.transform(loans_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_fixed(train_df, valid_df, test_df):\n",
    "    \"\"\"\n",
    "    Fixed Top-N one-hot + frequency encoding for categorical columns\n",
    "    (ServicerName, SellerName, PropertyState).\n",
    "    \"\"\"\n",
    "    top_cats = {\n",
    "        'ServicerName': [\n",
    "            'Other servicers', 'NATIONSTAR MORTGAGE LLC DBA MR. COOPER',\n",
    "            'AMERIHOME MORTGAGE COMPANY, LLC', 'UNITED WHOLESALE MORTGAGE, LLC',\n",
    "            'PENNYMAC LOAN SERVICES, LLC', 'LAKEVIEW LOAN SERVICING, LLC',\n",
    "            'ROCKET MORTGAGE, LLC', 'NEW RESIDENTIAL MORTGAGE LLC',\n",
    "            'JPMORGAN CHASE BANK, NATIONAL ASSOCIATION', 'FREEDOM MORTGAGE CORPORATION'\n",
    "        ],\n",
    "        'SellerName': [\n",
    "            'Other sellers', 'UNITED WHOLESALE MORTGAGE, LLC',\n",
    "            'AMERIHOME MORTGAGE COMPANY, LLC', 'ROCKET MORTGAGE, LLC',\n",
    "            'PENNYMAC LOAN SERVICES, LLC', 'NEWREZ LLC',\n",
    "            'JPMORGAN CHASE BANK, NATIONAL ASSOCIATION', 'U.S. BANK N.A.',\n",
    "            'PHH MORTGAGE CORPORATION', 'FAIRWAY INDEPENDENT MORTGAGE CORPORATION'\n",
    "        ],\n",
    "        'PropertyState': ['TX','FL','CA','NY','IL','OH','PA','MI','NC','GA']\n",
    "    }\n",
    "\n",
    "    def encode_column(df, train_df, col, top_list):\n",
    "        # frequency encoding\n",
    "        freq = train_df[col].value_counts(normalize=True).to_dict()\n",
    "        df[f\"{col}_freq\"] = df[col].map(freq).fillna(0)\n",
    "\n",
    "        # one-hot encoding for top-N\n",
    "        for cat in top_list:\n",
    "            df[f\"{col}_{cat[:15].replace(' ', '_')}\"] = (df[col] == cat).astype(int)\n",
    "\n",
    "        # Replace non-top values with \"Other\"\n",
    "        df[col] = df[col].where(df[col].isin(top_list), 'Other')\n",
    "\n",
    "    # copy to avoid modifying originals\n",
    "    train, valid, test = [df.copy() for df in (train_df, valid_df, test_df)]\n",
    "\n",
    "    # apply to each categorical column\n",
    "    for col, cats in top_cats.items():\n",
    "        print(f\"Encoding {col}...\")\n",
    "        for df in [train, valid, test]:\n",
    "            encode_column(df, train, col, cats)\n",
    "\n",
    "    return train, valid, test\n",
    "\n",
    "\n",
    "# Apply\n",
    "print(f\"Before encoding: Train {loans_train.shape}, Valid {loans_valid.shape}, Test {loans_test.shape}\")\n",
    "\n",
    "loans_train, loans_valid, loans_test = encode_categorical_fixed(loans_train, loans_valid, loans_test)\n",
    "\n",
    "print(f\"After encoding:  Train {loans_train.shape}, Valid {loans_valid.shape}, Test {loans_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61558c0",
   "metadata": {},
   "source": [
    "### panel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbd4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract statistical features from panel data time series\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "def extract_time_series_features(df, base_variables, time_points=14):\n",
    "    \"\"\"\n",
    "    Extract statistical features from panel data time series.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Extracting time series features for {len(base_variables)} variables...\")\n",
    "    print(f\"Base variables: {base_variables}\")\n",
    "    \n",
    "    # Create a copy to store results\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    for base_var in base_variables:\n",
    "        print(f\"\\nProcessing {base_var}...\")\n",
    "        \n",
    "        # Get all time-series columns for this variable\n",
    "        time_cols = [f\"{t}_{base_var}\" for t in range(time_points)]\n",
    "        existing_cols = [col for col in time_cols if col in df.columns]\n",
    "        \n",
    "        if not existing_cols:\n",
    "            print(f\"  Warning: No time-series columns found for {base_var}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Found {len(existing_cols)} time points\")\n",
    "        \n",
    "        # Check data type of the first column\n",
    "        first_col = existing_cols[0]\n",
    "        col_dtype = df[first_col].dtype\n",
    "        \n",
    "        if col_dtype == 'datetime64[ns]' or str(col_dtype).startswith('datetime'):\n",
    "            print(f\"  Skipping {base_var} - datetime columns cannot be used for statistical calculations\")\n",
    "            continue\n",
    "        \n",
    "        # Extract data for this variable across all time points\n",
    "        time_data = df[existing_cols].values  # Shape: (n_samples, n_time_points)\n",
    "        \n",
    "        # 1. Average Level\n",
    "        # Mean\n",
    "        result_df[f\"{base_var}_mean\"] = np.nanmean(time_data, axis=1)\n",
    "        # Standard deviation\n",
    "        result_df[f\"{base_var}_std\"] = np.nanstd(time_data, axis=1)\n",
    "        \n",
    "        # 2. Extreme Values\n",
    "        # Minimum\n",
    "        result_df[f\"{base_var}_min\"] = np.nanmin(time_data, axis=1)\n",
    "        # Maximum\n",
    "        result_df[f\"{base_var}_max\"] = np.nanmax(time_data, axis=1)\n",
    "        \n",
    "        # 3. Trend\n",
    "        # Linear regression slope\n",
    "        slopes = []\n",
    "        first_values = []\n",
    "        last_values = []\n",
    "        \n",
    "        for i in range(len(time_data)):\n",
    "            row_data = time_data[i]\n",
    "            valid_mask = ~np.isnan(row_data)\n",
    "            \n",
    "            if np.sum(valid_mask) >= 2:  # Need at least 2 points for slope\n",
    "                valid_data = row_data[valid_mask]\n",
    "                time_indices = np.arange(len(row_data))[valid_mask]\n",
    "                \n",
    "                # Calculate slope using linear regression\n",
    "                slope, _, _, _, _ = stats.linregress(time_indices, valid_data)\n",
    "                slopes.append(slope)\n",
    "                \n",
    "                # First and last values\n",
    "                first_values.append(valid_data[0])\n",
    "                last_values.append(valid_data[-1])\n",
    "            else:\n",
    "                slopes.append(np.nan)\n",
    "                first_values.append(np.nan)\n",
    "                last_values.append(np.nan)\n",
    "        \n",
    "        result_df[f\"{base_var}_slope\"] = slopes\n",
    "        result_df[f\"{base_var}_first\"] = first_values\n",
    "        result_df[f\"{base_var}_last\"] = last_values\n",
    "        \n",
    "        # 4. First-Last Difference\n",
    "        result_df[f\"{base_var}_diff\"] = result_df[f\"{base_var}_last\"] - result_df[f\"{base_var}_first\"]\n",
    "        \n",
    "        # 5. Volatility\n",
    "        # Range (max - min)\n",
    "        result_df[f\"{base_var}_range\"] = result_df[f\"{base_var}_max\"] - result_df[f\"{base_var}_min\"]\n",
    "        \n",
    "        print(f\"  Added 8 statistical features for {base_var}\")\n",
    "    \n",
    "    # Show summary of new features\n",
    "    original_cols = set(df.columns)\n",
    "    new_cols = set(result_df.columns)\n",
    "    added_cols = new_cols - original_cols\n",
    "    \n",
    "    print(f\"\\n=== Feature Extraction Summary ===\")\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    print(f\"New shape: {result_df.shape}\")\n",
    "    print(f\"Features added: {len(added_cols)}\")\n",
    "    print(f\"Expected features: {len(base_variables) * 8}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Define the 7 panel variables to extract features from (excluding MonthlyReportingPeriod)\n",
    "panel_variables = [\n",
    "    'CurrentActualUPB',\n",
    "    'CurrentInterestRate', \n",
    "    'CurrentNonInterestBearingUPB',\n",
    "    'EstimatedLTV',\n",
    "    'InterestBearingUPB',\n",
    "    'LoanAge',\n",
    "    'RemainingMonthsToLegalMaturity'\n",
    "]\n",
    "\n",
    "print(\"=== Extracting Time Series Features ===\")\n",
    "print(f\"Panel variables: {panel_variables}\")\n",
    "print(f\"Note: MonthlyReportingPeriod skipped (datetime type)\")\n",
    "print(f\"Expected features per variable: 8\")\n",
    "print(f\"Total expected new features: {len(panel_variables) * 8}\")\n",
    "\n",
    "# Extract features for training data\n",
    "print(f\"\\n--- Processing Training Data ---\")\n",
    "print(f\"Before: {loans_train.shape}\")\n",
    "loans_train_ts = extract_time_series_features(loans_train, panel_variables)\n",
    "print(f\"After: {loans_train_ts.shape}\")\n",
    "\n",
    "# Extract features for validation data\n",
    "print(f\"\\n--- Processing Validation Data ---\")\n",
    "print(f\"Before: {loans_valid.shape}\")\n",
    "loans_valid_ts = extract_time_series_features(loans_valid, panel_variables)\n",
    "print(f\"After: {loans_valid_ts.shape}\")\n",
    "\n",
    "# Extract features for test data\n",
    "print(f\"\\n--- Processing Test Data ---\")\n",
    "print(f\"Before: {loans_test.shape}\")\n",
    "loans_test_ts = extract_time_series_features(loans_test, panel_variables)\n",
    "print(f\"After: {loans_test_ts.shape}\")\n",
    "\n",
    "# Update the original dataframes\n",
    "loans_train = loans_train_ts\n",
    "loans_valid = loans_valid_ts\n",
    "loans_test = loans_test_ts\n",
    "\n",
    "print(f\"\\n=== Final Dataset Summary ===\")\n",
    "print(f\"Train: {loans_train.shape}\")\n",
    "print(f\"Valid: {loans_valid.shape}\")\n",
    "print(f\"Test: {loans_test.shape}\")\n",
    "\n",
    "# Show sample of new features\n",
    "print(f\"\\n=== Sample of New Time Series Features ===\")\n",
    "ts_features = [col for col in loans_train.columns if any(col.endswith(suffix) for suffix in ['_mean', '_std', '_min', '_max', '_slope', '_diff', '_first', '_last', '_range'])]\n",
    "print(f\"Total time series features: {len(ts_features)}\")\n",
    "print(f\"Sample features: {ts_features[:10]}\")\n",
    "\n",
    "# Show sample data\n",
    "print(f\"\\nSample data for CurrentActualUPB features:\")\n",
    "upb_features = [col for col in loans_train.columns if col.startswith('CurrentActualUPB_')]\n",
    "print(loans_train[upb_features].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d8469",
   "metadata": {},
   "source": [
    "### Statistic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2749e393",
   "metadata": {},
   "source": [
    "***Normalization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5075cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns that need scaling\n",
    "print(\"=== Identifying Columns for Scaling ===\")\n",
    "\n",
    "# Get all columns\n",
    "all_cols = loans_train.columns.tolist()\n",
    "numeric_cols = loans_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Exclude columns that shouldn't be scaled\n",
    "exclude_cols = ['index', 'target']\n",
    "binary_cols = []\n",
    "\n",
    "# Identify binary columns (0/1 only)\n",
    "for col in numeric_cols:\n",
    "    if col not in exclude_cols:\n",
    "        unique_vals = set(loans_train[col].dropna().unique())\n",
    "        if unique_vals.issubset({0, 1}):\n",
    "            binary_cols.append(col)\n",
    "\n",
    "# Columns to scale = numeric - binary - excluded\n",
    "scale_cols = [col for col in numeric_cols if col not in binary_cols + exclude_cols]\n",
    "\n",
    "# Feature Scaling using the identified columns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def apply_feature_scaling(train_df, valid_df, test_df, scale_cols):\n",
    "    \"\"\"\n",
    "    Apply feature scaling to identified numerical features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create copies\n",
    "    train_scaled = train_df.copy()\n",
    "    valid_scaled = valid_df.copy()\n",
    "    test_scaled = test_df.copy()\n",
    "    \n",
    "    # Initialize scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    train_scaled[scale_cols] = scaler.fit_transform(train_df[scale_cols])\n",
    "    valid_scaled[scale_cols] = scaler.transform(valid_df[scale_cols])\n",
    "    test_scaled[scale_cols] = scaler.transform(test_df[scale_cols])\n",
    "    \n",
    "    print(\"âœ… Feature scaling completed!\")\n",
    "    return train_scaled, valid_scaled, test_scaled, scaler\n",
    "\n",
    "# Apply scaling to all datasets\n",
    "loans_train_scaled, loans_valid_scaled, loans_test_scaled, feature_scaler = apply_feature_scaling(\n",
    "    loans_train, loans_valid, loans_test, scale_cols\n",
    ")\n",
    "\n",
    "# Update original dataframes\n",
    "loans_train = loans_train_scaled\n",
    "loans_valid = loans_valid_scaled\n",
    "loans_test = loans_test_scaled\n",
    "\n",
    "print(f\"\\n=== Final Scaled Dataset Summary ===\")\n",
    "print(f\"Train: {loans_train.shape}\")\n",
    "print(f\"Valid: {loans_valid.shape}\")\n",
    "print(f\"Test: {loans_test.shape}\")\n",
    "\n",
    "# Verify scaling results\n",
    "print(f\"\\n=== Scaling Verification ===\")\n",
    "print(\"Sample scaled features (mean â‰ˆ 0, std â‰ˆ 1):\")\n",
    "sample_features = scale_cols[:5]\n",
    "for col in sample_features:\n",
    "    mean_val = loans_train[col].mean()\n",
    "    std_val = loans_train[col].std()\n",
    "    print(f\"{col}: mean={mean_val:.4f}, std={std_val:.4f}\")\n",
    "\n",
    "# Show feature importance info\n",
    "print(f\"\\n=== Feature Scaling Summary ===\")\n",
    "print(f\"âœ… Successfully scaled {len(scale_cols)} numerical features\")\n",
    "print(f\"âœ… Preserved {len(binary_cols)} binary features (not scaled)\")\n",
    "print(f\"âœ… Excluded {len(exclude_cols)} special columns (index, target)\")\n",
    "print(f\"âœ… Total features processed: {len(loans_train.columns)}\")\n",
    "\n",
    "# Show which features were scaled\n",
    "print(f\"\\n=== Scaled Features Categories ===\")\n",
    "print(f\"ðŸ“Š Original features (13): {[col for col in scale_cols if not any(col.startswith(prefix) for prefix in ['0_', '1_', '2_', '3_', '4_', '5_', '6_', '7_', '8_', '9_', '10_', '11_', '12_', '13_', 'Current', 'Servicer', 'Seller', 'Property'])][:13]}\")\n",
    "print(f\"ðŸ“ˆ Time series features (98): {len([col for col in scale_cols if any(col.startswith(prefix) for prefix in ['0_', '1_', '2_', '3_', '4_', '5_', '6_', '7_', '8_', '9_', '10_', '11_', '12_', '13_'])])} features\")\n",
    "print(f\"ðŸ“Š Frequency features (3): {[col for col in scale_cols if col.endswith('_freq')]}\")\n",
    "print(f\"ðŸ“Š Statistical features (36): {len([col for col in scale_cols if any(col.endswith(suffix) for suffix in ['_mean', '_std', '_min', '_max', '_slope', '_first', '_last', '_diff', '_range'])])} features\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Scaler object saved as 'feature_scaler' for later use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c9cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed datasets to data/feature_engineering directory\n",
    "import os\n",
    "\n",
    "# Create directory and save datasets\n",
    "output_dir = \"data/feature_engineering\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save datasets\n",
    "loans_train.to_csv(os.path.join(output_dir, \"loans_train_processed.csv\"), index=False)\n",
    "loans_valid.to_csv(os.path.join(output_dir, \"loans_valid_processed.csv\"), index=False)\n",
    "loans_test.to_csv(os.path.join(output_dir, \"loans_test_processed.csv\"), index=False)\n",
    "\n",
    "print(f\"âœ… Saved 3 datasets + scaler to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef8ad8",
   "metadata": {},
   "source": [
    "# Training Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90347c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "train = pd.read_csv(\"data/feature_engineering/loans_train_processed.csv\")\n",
    "test = pd.read_csv(\"data/feature_engineering/loans_test_processed.csv\")\n",
    "valid = pd.read_csv(\"data/feature_engineering/loans_valid_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa49969",
   "metadata": {},
   "source": [
    "### training model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Datetime Columns Before Training\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_datetime_columns(df):\n",
    "    \"\"\"Remove or convert datetime columns to numeric features\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Find datetime columns\n",
    "    datetime_cols = df_copy.select_dtypes(include=['datetime64[ns]', 'object']).columns\n",
    "    \n",
    "    # Check for datetime-like strings in object columns\n",
    "    for col in df_copy.columns:\n",
    "        if df_copy[col].dtype == 'object':\n",
    "            try:\n",
    "                pd.to_datetime(df_copy[col].iloc[0])\n",
    "                datetime_cols = datetime_cols.append(pd.Index([col]))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    print(f\"Found datetime columns: {list(datetime_cols)}\")\n",
    "    \n",
    "    # Extract numeric features from datetime columns\n",
    "    for col in datetime_cols:\n",
    "        if col in df_copy.columns:\n",
    "            try:\n",
    "                # Convert to datetime if it's a string\n",
    "                if df_copy[col].dtype == 'object':\n",
    "                    df_copy[col] = pd.to_datetime(df_copy[col], errors='coerce')\n",
    "                \n",
    "                # Extract year, month, day\n",
    "                df_copy[f'{col}_year'] = df_copy[col].dt.year\n",
    "                df_copy[f'{col}_month'] = df_copy[col].dt.month\n",
    "                df_copy[f'{col}_day'] = df_copy[col].dt.day\n",
    "                \n",
    "                # Drop original datetime column\n",
    "                df_copy.drop(columns=[col], inplace=True)\n",
    "                \n",
    "            except:\n",
    "                # If conversion fails, just drop the column\n",
    "                df_copy.drop(columns=[col], inplace=True)\n",
    "                print(f\"Dropped problematic column: {col}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Clean all datasets\n",
    "train = clean_datetime_columns(train)\n",
    "valid = clean_datetime_columns(valid)\n",
    "test = clean_datetime_columns(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9099926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract YYYY, MM, DD from Datetime Columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_datetime_features(df):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for col in ['FirstPaymentDate', 'MaturityDate']:\n",
    "        if col in df_copy.columns:\n",
    "            # Safely convert to datetime if not already\n",
    "            df_copy[col] = pd.to_datetime(df_copy[col], errors='coerce')\n",
    "\n",
    "            # Extract components using datetime accessor\n",
    "            df_copy[col + '_year'] = df_copy[col].dt.year\n",
    "            df_copy[col + '_month'] = df_copy[col].dt.month\n",
    "            df_copy[col + '_day'] = df_copy[col].dt.day\n",
    "\n",
    "            # Drop original datetime column\n",
    "            df_copy.drop(columns=[col], inplace=True)\n",
    "            print(f\"Extracted year, month, day from {col}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Apply extraction to all datasets\n",
    "train = extract_datetime_features(train)\n",
    "valid = extract_datetime_features(valid)\n",
    "test  = extract_datetime_features(test)\n",
    "\n",
    "print(\"\\nâœ… Datetime extraction completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f59ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop all original panel columns (0_...13_...)\n",
    "panel_cols = [c for c in train.columns if any(c.startswith(f\"{i}_\") for i in range(14))]\n",
    "train = train.drop(columns=panel_cols, errors='ignore')\n",
    "valid = valid.drop(columns=panel_cols, errors='ignore')\n",
    "test  = test.drop(columns=panel_cols,  errors='ignore')\n",
    "\n",
    "# 2. Keep aggregated statistical features\n",
    "# (e.g. EstimatedLTV_mean, EstimatedLTV_slope, LoanAge_range, etc.)\n",
    "X_train = train.drop(['index', 'target'], axis=1)\n",
    "y_train = train['target']\n",
    "X_valid = valid.drop(['index', 'target'], axis=1)\n",
    "y_valid = valid['target']\n",
    "X_test  = test.drop(['Id'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Cleaned Datasets for Model Training\n",
    "\n",
    "from signal import valid_signals\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Use your cleaned datasets that already have datetime features extracted\n",
    "X_train = train.drop(columns=['index', 'target'], errors='ignore')\n",
    "y_train = train['target']\n",
    "X_valid = valid.drop(columns=['index', 'target'], errors='ignore')\n",
    "y_valid = valid['target']\n",
    "X_test = test.drop(columns=['Id'], errors='ignore')\n",
    "\n",
    "# Ensure all datasets have the same columns\n",
    "common_cols = set(X_train.columns) & set(X_valid.columns) & set(X_test.columns)\n",
    "X_train = X_train[list(common_cols)]\n",
    "X_valid = X_valid[list(common_cols)]\n",
    "X_test = X_test[list(common_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38bd3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58441ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill remaining NaN values using robust methods\n",
    "\n",
    "print(\"Step 2: Fill remaining NaN values\")\n",
    "\n",
    "# For numeric columns, use median imputation\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if X_train[col].isna().any():\n",
    "        median_val = X_train[col].median()\n",
    "        if pd.isna(median_val):  # If median is also NaN, use 0\n",
    "            median_val = 0\n",
    "        X_train[col] = X_train[col].fillna(median_val)\n",
    "        X_valid[col] = X_valid[col].fillna(median_val)\n",
    "        X_test[col] = X_test[col].fillna(median_val)\n",
    "\n",
    "# For categorical columns, use mode imputation\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_cols:\n",
    "    if X_train[col].isna().any():\n",
    "        mode_val = X_train[col].mode()\n",
    "        if len(mode_val) > 0:\n",
    "            X_train[col] = X_train[col].fillna(mode_val[0])\n",
    "            X_valid[col] = X_valid[col].fillna(mode_val[0])\n",
    "            X_test[col] = X_test[col].fillna(mode_val[0])\n",
    "        else:\n",
    "            # If no mode exists, use 'unknown'\n",
    "            X_train[col] = X_train[col].fillna('unknown')\n",
    "            X_valid[col] = X_valid[col].fillna('unknown')\n",
    "            X_test[col] = X_test[col].fillna('unknown')\n",
    "\n",
    "# 3. Verify the fix results\n",
    "print(\"Step 3: Verify fix results\")\n",
    "print(f\"NaN count in X_train: {X_train.isna().sum().sum()}\")\n",
    "print(f\"NaN count in X_valid: {X_valid.isna().sum().sum()}\")\n",
    "print(f\"NaN count in X_test: {X_test.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7bf3e",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af8da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import roc_auc_score, precision_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=== Simplified Unsupervised Baseline (AUC + Precision + ROC) ===\")\n",
    "print(f\"Data: X_train={X_train.shape}, X_valid={X_valid.shape}, X_test={X_test.shape}\")\n",
    "\n",
    "# --- Define Models ---\n",
    "models = {\n",
    "    \"IsolationForest\": IsolationForest(n_estimators=300, contamination=0.1, random_state=42),\n",
    "    \"PCA\": PCA(n_components=0.95, random_state=42),\n",
    "    \"LOF\": LocalOutlierFactor(n_neighbors=20, contamination=0.1, novelty=True)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# --- Train & Evaluate ---\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "\n",
    "    if name == \"PCA\":\n",
    "        model.fit(X_train)\n",
    "        valid_recon = model.inverse_transform(model.transform(X_valid))\n",
    "        scores = np.mean((X_valid - valid_recon) ** 2, axis=1)\n",
    "    else:\n",
    "        model.fit(X_train)\n",
    "        scores = -model.decision_function(X_valid)\n",
    "\n",
    "    # AUC\n",
    "    auc = roc_auc_score(y_valid, scores)\n",
    "\n",
    "    # Precision using top 5% threshold\n",
    "    threshold = np.percentile(scores, 95)\n",
    "    y_pred = (scores >= threshold).astype(int)\n",
    "    precision = precision_score(y_valid, y_pred, zero_division=0)\n",
    "\n",
    "    print(f\"AUC = {auc:.4f} | Precision@95% = {precision:.4f}\")\n",
    "    results[name] = (scores, auc, precision)\n",
    "\n",
    "# --- Plot ROC Curves ---\n",
    "plt.figure(figsize=(7, 6))\n",
    "for name, (scores, auc, _) in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_valid, scores)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves - Unsupervised Baseline\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8221ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== X_train Columns and Data Types ===\")\n",
    "for col in X_train.columns:\n",
    "    print(f\"{col:50s} {X_train[col].dtype}\")\n",
    "print(f\"\\nTotal columns: {len(X_train.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "is5126",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
