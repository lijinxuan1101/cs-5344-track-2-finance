{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f605c3d3",
   "metadata": {},
   "source": [
    "# Training Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4313a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b6d1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/feature_engineering/loans_train.csv\")\n",
    "test = pd.read_csv(\"data/feature_engineering/loans_test.csv\")\n",
    "valid = pd.read_csv(\"data/feature_engineering/loans_valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc5cc6",
   "metadata": {},
   "source": [
    "### training model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8bbc8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Datetime Columns Before Training (concise)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATE_COLS = ['FirstPaymentDate', 'MaturityDate']\n",
    "\n",
    "def extract_true_datetime_parts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dfc = df.copy()\n",
    "    for col in DATE_COLS:\n",
    "        if col in dfc.columns:\n",
    "            dfc[col] = pd.to_datetime(dfc[col], format='%Y-%m-%d', errors='coerce')\n",
    "            dfc[f'{col}_year'] = dfc[col].dt.year\n",
    "            dfc[f'{col}_month'] = dfc[col].dt.month\n",
    "            dfc.drop(columns=[col], inplace=True)\n",
    "    return dfc\n",
    "\n",
    "train = extract_true_datetime_parts(train)\n",
    "valid = extract_true_datetime_parts(valid)\n",
    "test  = extract_true_datetime_parts(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a14197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Datetime extraction completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Extract YYYY, MM, DD from Datetime Columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_datetime_features(df):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for col in ['FirstPaymentDate', 'MaturityDate']:\n",
    "        if col in df_copy.columns:\n",
    "            # Safely convert to datetime if not already\n",
    "            df_copy[col] = pd.to_datetime(df_copy[col], errors='coerce')\n",
    "\n",
    "            # Extract components using datetime accessor\n",
    "            df_copy[col + '_year'] = df_copy[col].dt.year\n",
    "            df_copy[col + '_month'] = df_copy[col].dt.month\n",
    "            df_copy[col + '_day'] = df_copy[col].dt.day\n",
    "\n",
    "            # Drop original datetime column\n",
    "            df_copy.drop(columns=[col], inplace=True)\n",
    "            print(f\"Extracted year, month, day from {col}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Apply extraction to all datasets\n",
    "train = extract_datetime_features(train)\n",
    "valid = extract_datetime_features(valid)\n",
    "test  = extract_datetime_features(test)\n",
    "\n",
    "print(\"\\n✅ Datetime extraction completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "229e4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop all original panel columns (0_...13_...)\n",
    "panel_cols = [c for c in train.columns if any(c.startswith(f\"{i}_\") for i in range(14))]\n",
    "train = train.drop(columns=panel_cols, errors='ignore')\n",
    "valid = valid.drop(columns=panel_cols, errors='ignore')\n",
    "test  = test.drop(columns=panel_cols,  errors='ignore')\n",
    "\n",
    "# 2. Keep aggregated statistical features\n",
    "# (e.g. EstimatedLTV_mean, EstimatedLTV_slope, LoanAge_range, etc.)\n",
    "X_train = train.drop(['index', 'target'], axis=1)\n",
    "y_train = train['target']\n",
    "X_valid = valid.drop(['index', 'target'], axis=1)\n",
    "y_valid = valid['target']\n",
    "X_test  = test.drop(['Id'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72778c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Cleaned Datasets for Model Training\n",
    "\n",
    "from signal import valid_signals\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Use your cleaned datasets that already have datetime features extracted\n",
    "X_train = train.drop(columns=['index', 'target'], errors='ignore')\n",
    "y_train = train['target']\n",
    "X_valid = valid.drop(columns=['index', 'target'], errors='ignore')\n",
    "y_valid = valid['target']\n",
    "X_test = test.drop(columns=['Id'], errors='ignore')\n",
    "\n",
    "# Ensure all datasets have the same columns\n",
    "common_cols = set(X_train.columns) & set(X_valid.columns) & set(X_test.columns)\n",
    "X_train = X_train[list(common_cols)]\n",
    "X_valid = X_valid[list(common_cols)]\n",
    "X_test = X_test[list(common_cols)]\n",
    "\n",
    "# Drop all string/categorical columns (union across splits)\n",
    "obj_cols = set(X_train.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols |= set(X_valid.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols |= set(X_test.select_dtypes(include=['object', 'category']).columns)\n",
    "obj_cols = list(obj_cols)\n",
    "\n",
    "X_train = X_train.drop(columns=obj_cols, errors='ignore')\n",
    "X_valid = X_valid.drop(columns=obj_cols, errors='ignore')\n",
    "X_test  = X_test.drop(columns=obj_cols, errors='ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f07df095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Fill remaining NaN values\n",
      "Step 3: Verify fix results\n",
      "NaN count in X_train: 0\n",
      "NaN count in X_valid: 0\n",
      "NaN count in X_test: 0\n"
     ]
    }
   ],
   "source": [
    "# Fill remaining NaN values using robust methods\n",
    "\n",
    "print(\"Step 2: Fill remaining NaN values\")\n",
    "\n",
    "# For numeric columns, use median imputation\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if X_train[col].isna().any():\n",
    "        median_val = X_train[col].median()\n",
    "        if pd.isna(median_val):  # If median is also NaN, use 0\n",
    "            median_val = 0\n",
    "        X_train[col] = X_train[col].fillna(median_val)\n",
    "        X_valid[col] = X_valid[col].fillna(median_val)\n",
    "        X_test[col] = X_test[col].fillna(median_val)\n",
    "\n",
    "# For categorical columns, use mode imputation\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_cols:\n",
    "    if X_train[col].isna().any():\n",
    "        mode_val = X_train[col].mode()\n",
    "        if len(mode_val) > 0:\n",
    "            X_train[col] = X_train[col].fillna(mode_val[0])\n",
    "            X_valid[col] = X_valid[col].fillna(mode_val[0])\n",
    "            X_test[col] = X_test[col].fillna(mode_val[0])\n",
    "        else:\n",
    "            # If no mode exists, use 'unknown'\n",
    "            X_train[col] = X_train[col].fillna('unknown')\n",
    "            X_valid[col] = X_valid[col].fillna('unknown')\n",
    "            X_test[col] = X_test[col].fillna('unknown')\n",
    "\n",
    "# 3. Verify the fix results\n",
    "print(\"Step 3: Verify fix results\")\n",
    "print(f\"NaN count in X_train: {X_train.isna().sum().sum()}\")\n",
    "print(f\"NaN count in X_valid: {X_valid.isna().sum().sum()}\")\n",
    "print(f\"NaN count in X_test: {X_test.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8f2ac",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d494f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 确保都是 DataFrame\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_valid = pd.DataFrame(X_valid)\n",
    "X_test  = pd.DataFrame(X_test)\n",
    "\n",
    "# 列顺序对齐\n",
    "X_valid = X_valid[X_train.columns]\n",
    "X_test  = X_test[X_train.columns]\n",
    "\n",
    "# 保证类别特征为 string，数值特征为 float\n",
    "for df in [X_train, X_valid, X_test]:\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            df[c] = df[c].astype(float)\n",
    "        else:\n",
    "            df[c] = df[c].astype(str).fillna(\"missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73c8b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "303599e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFOD:\n",
    "    \"\"\"\n",
    "    RFOD: Random Forest-based Outlier Detection for Tabular Data\n",
    "    - Feature-wise conditional modeling\n",
    "    - OOB-based tree pruning (MODIFIED TO INCLUDE)\n",
    "    - Adjusted Gower’s Distance (AGD)\n",
    "    - Uncertainty Weighted Averaging (UWA)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_estimators=300, max_depth=None, beta=0.5, alpha=0.02, n_jobs=-1, random_state=42):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.beta = beta                  # keep top β fraction of trees\n",
    "        self.alpha = alpha                # quantile for AGD normalization\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.models = {}                  # store per-feature models\n",
    "\n",
    "    # ===============================\n",
    "    # Helper: detect numeric/categorical\n",
    "    # ===============================\n",
    "    def _split_features(self, X):\n",
    "        num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "        cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "        return num_cols, cat_cols\n",
    "\n",
    "    # ===============================\n",
    "    # Fit phase\n",
    "    # ===============================\n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Train a separate RF for each feature using leave-one-feature-out scheme.\n",
    "        Includes OOB-based Forest Pruning.\n",
    "        \"\"\"\n",
    "        self.num_cols, self.cat_cols = self._split_features(X)\n",
    "        self.models = {}\n",
    "        \n",
    "        # OOB 评估需要这些指标\n",
    "        # 确保它们已在 Cell [36] 中导入\n",
    "        # from sklearn.metrics import r2_score, accuracy_score\n",
    "\n",
    "        for col in X.columns:\n",
    "            X_others = X.drop(columns=[col])\n",
    "            y = X[col]\n",
    "            \n",
    "            all_indices = set(range(len(X_others)))\n",
    "\n",
    "            # train numeric or categorical RF\n",
    "            if col in self.num_cols:\n",
    "                model = RandomForestRegressor(\n",
    "                    n_estimators=self.n_estimators,\n",
    "                    max_depth=self.max_depth,\n",
    "                    n_jobs=self.n_jobs,\n",
    "                    random_state=self.random_state,\n",
    "                    bootstrap=True,\n",
    "                    oob_score=True\n",
    "                )\n",
    "                model.fit(X_others, y)\n",
    "                \n",
    "                # === MODIFICATION: START FOREST PRUNING (Module 2) ===\n",
    "                trees = model.estimators_\n",
    "                in_bag_samples = model.estimators_samples_\n",
    "                tree_scores = []\n",
    "\n",
    "                for i, tree in enumerate(trees):\n",
    "                    oob_idx = list(all_indices - set(in_bag_samples[i]))\n",
    "                    if not oob_idx:\n",
    "                        tree_scores.append(-np.inf)\n",
    "                        continue\n",
    "                    \n",
    "                    X_oob = X_others.iloc[oob_idx]\n",
    "                    y_oob = y.iloc[oob_idx]\n",
    "                    \n",
    "                    try:\n",
    "                        y_pred_oob = tree.predict(X_oob)\n",
    "                        # 论文建议使用 R^2 (coefficient of determination) [cite: 224]\n",
    "                        score = r2_score(y_oob, y_pred_oob)\n",
    "                        tree_scores.append(score)\n",
    "                    except ValueError:\n",
    "                        tree_scores.append(-np.inf) # Handle errors\n",
    "\n",
    "                sorted_indices = np.argsort(tree_scores)[::-1]\n",
    "                num_to_keep = int(np.floor(self.beta * len(trees)))\n",
    "                top_indices = sorted_indices[:num_to_keep]\n",
    "                pruned_trees = [trees[i] for i in top_indices]\n",
    "                # === MODIFICATION: END FOREST PRUNING ===\n",
    "                \n",
    "                q_low, q_high = y.quantile([self.alpha, 1 - self.alpha])\n",
    "                \n",
    "                # MODIFICATION: 存储剪枝后的树列表，而不是完整的模型\n",
    "                self.models[col] = {\"type\": \"num\", \"model\": pruned_trees, \"q_low\": q_low, \"q_high\": q_high}\n",
    "\n",
    "            else:\n",
    "                le = LabelEncoder()\n",
    "                y_enc = le.fit_transform(y.astype(str))\n",
    "                model = RandomForestClassifier(\n",
    "                    n_estimators=self.n_estimators,\n",
    "                    max_depth=self.max_depth,\n",
    "                    n_jobs=self.n_jobs,\n",
    "                    random_state=self.random_state,\n",
    "                    bootstrap=True,\n",
    "                    oob_score=True\n",
    "                )\n",
    "                model.fit(X_others, y_enc)\n",
    "                \n",
    "                # === MODIFICATION: START FOREST PRUNING (Module 2) ===\n",
    "                trees = model.estimators_\n",
    "                in_bag_samples = model.estimators_samples_\n",
    "                tree_scores = []\n",
    "                \n",
    "                for i, tree in enumerate(trees):\n",
    "                    oob_idx = list(all_indices - set(in_bag_samples[i]))\n",
    "                    if not oob_idx:\n",
    "                        tree_scores.append(-np.inf)\n",
    "                        continue\n",
    "\n",
    "                    X_oob = X_others.iloc[oob_idx]\n",
    "                    y_oob_enc = y_enc[oob_idx] # 使用编码后\n",
    "                    \n",
    "                    try:\n",
    "                        y_pred_oob = tree.predict(X_oob)\n",
    "                        # 论文建议使用 AUC-ROC，但 Accuracy 更简单且已导入 [cite: 224]\n",
    "                        score = accuracy_score(y_oob_enc, y_pred_oob)\n",
    "                        tree_scores.append(score)\n",
    "                    except ValueError:\n",
    "                        tree_scores.append(-np.inf)\n",
    "                        \n",
    "                sorted_indices = np.argsort(tree_scores)[::-1]\n",
    "                num_to_keep = int(np.floor(self.beta * len(trees)))\n",
    "                top_indices = sorted_indices[:num_to_keep]\n",
    "                pruned_trees = [trees[i] for i in top_indices]\n",
    "                # === MODIFICATION: END FOREST PRUNING ===\n",
    "\n",
    "                # MODIFICATION: 存储剪枝后的树列表\n",
    "                self.models[col] = {\"type\": \"cat\", \"model\": pruned_trees, \"encoder\": le}\n",
    "\n",
    "        return self\n",
    "\n",
    "    # ===============================\n",
    "    # Predict phase\n",
    "    # ===============================\n",
    "    def _predict_numeric(self, model, X, y_true, q_low, q_high):\n",
    "        # MODIFICATION: 'model' 现在是树的列表，不再是RF对象\n",
    "        trees = model \n",
    "        preds = np.column_stack([t.predict(X) for t in trees])\n",
    "        mean_pred = preds.mean(axis=1)\n",
    "        std_pred = preds.std(axis=1)\n",
    "        agd = np.abs(y_true - mean_pred) / (q_high - q_low + 1e-9)\n",
    "        return agd, std_pred\n",
    "\n",
    "    def _predict_categorical(self, model, le, X, y_true):\n",
    "        y_true_enc = le.transform(y_true.astype(str))\n",
    "        \n",
    "        # MODIFICATION: 'model' 现在是树的列表\n",
    "        trees = model \n",
    "\n",
    "        # per-tree correctness variance (uncertainty)\n",
    "        tree_preds = np.column_stack([t.predict(X) for t in trees])\n",
    "        correct = (tree_preds == y_true_enc[:, None]).astype(float)\n",
    "        std_uncert = correct.std(axis=1)\n",
    "\n",
    "        # mean prob of true class\n",
    "        avg_proba = np.mean([t.predict_proba(X) for t in trees], axis=0)\n",
    "        p_true = avg_proba[np.arange(len(y_true_enc)), y_true_enc]\n",
    "        agd = 1 - p_true\n",
    "        return agd, std_uncert\n",
    "\n",
    "    # ===============================\n",
    "    # Compute anomaly scores\n",
    "    # ===============================\n",
    "    def _compute_scores(self, X: pd.DataFrame):\n",
    "        S = pd.DataFrame(np.zeros_like(X, dtype=float), columns=X.columns)\n",
    "        U = pd.DataFrame(np.zeros_like(X, dtype=float), columns=X.columns)\n",
    "\n",
    "        for col in X.columns:\n",
    "            model_info = self.models[col]\n",
    "            X_others = X.drop(columns=[col])\n",
    "            y_true = X[col]\n",
    "\n",
    "            if model_info[\"type\"] == \"num\":\n",
    "                agd, uncert = self._predict_numeric(\n",
    "                    model_info[\"model\"], X_others, y_true.astype(float),\n",
    "                    model_info[\"q_low\"], model_info[\"q_high\"]\n",
    "                )\n",
    "            else:\n",
    "                agd, uncert = self._predict_categorical(\n",
    "                    model_info[\"model\"], model_info[\"encoder\"], X_others, y_true\n",
    "                )\n",
    "\n",
    "            S[col] = agd\n",
    "            U[col] = uncert\n",
    "\n",
    "        return S, U\n",
    "\n",
    "    # ===============================\n",
    "    # Public APIs\n",
    "    # ===============================\n",
    "    def cell_anomaly_scores(self, X):\n",
    "        S, _ = self._compute_scores(X)\n",
    "        return S\n",
    "\n",
    "    def score_samples(self, X):\n",
    "        S, U = self._compute_scores(X)\n",
    "        U_sum = U.sum(axis=1).replace(0, np.finfo(float).eps)\n",
    "        U_norm = U.div(U_sum, axis=0)\n",
    "        W = 1 - U_norm\n",
    "        s_row = (W.values * S.values).mean(axis=1)\n",
    "        return s_row\n",
    "\n",
    "    def fit_score(self, X_train_normal, X_test):\n",
    "        self.fit(X_train_normal)\n",
    "        S = self.cell_anomaly_scores(X_test)\n",
    "        s = self.score_samples(X_test)\n",
    "        return S, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd64a052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC-AUC=0.5609, AP=0.1956\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# 忽略这个特定的\"feature names\"警告 (UserWarning 是 Python 内置的)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*X has feature names.*\")\n",
    "\n",
    "\n",
    "rfod = RFOD(n_estimators=150, beta=0.5, alpha=0.02, n_jobs=-1, random_state=42)\n",
    "\n",
    "# 用正常样本训练\n",
    "X_train_normal = X_train[y_train == 0]\n",
    "rfod.fit(X_train_normal)\n",
    "\n",
    "# 验证集\n",
    "S_valid = rfod.cell_anomaly_scores(X_valid)\n",
    "s_valid = rfod.score_samples(X_valid)\n",
    "\n",
    "# 评估\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "roc = roc_auc_score(y_valid, s_valid)\n",
    "ap = average_precision_score(y_valid, s_valid)\n",
    "print(f\"Validation ROC-AUC={roc:.4f}, AP={ap:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b50b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_test, s_test = rfod.fit_score(X_train_normal, X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": np.arange(len(s_test)),\n",
    "    \"target\": s_test\n",
    "})\n",
    "submission.to_csv(\"submission_stage2.csv\", index=False)\n",
    "print(submission.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "is5126",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
